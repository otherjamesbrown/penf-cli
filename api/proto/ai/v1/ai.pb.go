// AI Coordinator Service Protobuf Definitions
// Defines the gRPC service for AI/ML operations including embeddings,
// summarization, assertion extraction, and content classification.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v6.33.4
// source: api/proto/ai/v1/ai.proto

package aiv1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// SummaryStyle defines the format and tone of the generated summary.
type SummaryStyle int32

const (
	// Default style - balanced and informative.
	SummaryStyle_SUMMARY_STYLE_UNSPECIFIED SummaryStyle = 0
	// Brief, executive-style summary focusing on key points.
	SummaryStyle_SUMMARY_STYLE_BRIEF SummaryStyle = 1
	// Detailed summary preserving more context and nuance.
	SummaryStyle_SUMMARY_STYLE_DETAILED SummaryStyle = 2
	// Bullet-point format highlighting main topics.
	SummaryStyle_SUMMARY_STYLE_BULLET_POINTS SummaryStyle = 3
	// Technical summary focusing on facts and data.
	SummaryStyle_SUMMARY_STYLE_TECHNICAL SummaryStyle = 4
)

// Enum value maps for SummaryStyle.
var (
	SummaryStyle_name = map[int32]string{
		0: "SUMMARY_STYLE_UNSPECIFIED",
		1: "SUMMARY_STYLE_BRIEF",
		2: "SUMMARY_STYLE_DETAILED",
		3: "SUMMARY_STYLE_BULLET_POINTS",
		4: "SUMMARY_STYLE_TECHNICAL",
	}
	SummaryStyle_value = map[string]int32{
		"SUMMARY_STYLE_UNSPECIFIED":   0,
		"SUMMARY_STYLE_BRIEF":         1,
		"SUMMARY_STYLE_DETAILED":      2,
		"SUMMARY_STYLE_BULLET_POINTS": 3,
		"SUMMARY_STYLE_TECHNICAL":     4,
	}
)

func (x SummaryStyle) Enum() *SummaryStyle {
	p := new(SummaryStyle)
	*p = x
	return p
}

func (x SummaryStyle) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (SummaryStyle) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proto_ai_v1_ai_proto_enumTypes[0].Descriptor()
}

func (SummaryStyle) Type() protoreflect.EnumType {
	return &file_api_proto_ai_v1_ai_proto_enumTypes[0]
}

func (x SummaryStyle) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use SummaryStyle.Descriptor instead.
func (SummaryStyle) EnumDescriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{0}
}

// ModelType identifies the category of AI model.
type ModelType int32

const (
	// Unknown or unspecified model type.
	ModelType_MODEL_TYPE_UNSPECIFIED ModelType = 0
	// Embedding model for vector generation.
	ModelType_MODEL_TYPE_EMBEDDING ModelType = 1
	// Large language model for text generation.
	ModelType_MODEL_TYPE_LLM ModelType = 2
	// Classification model for categorization.
	ModelType_MODEL_TYPE_CLASSIFIER ModelType = 3
	// Named entity recognition model.
	ModelType_MODEL_TYPE_NER ModelType = 4
)

// Enum value maps for ModelType.
var (
	ModelType_name = map[int32]string{
		0: "MODEL_TYPE_UNSPECIFIED",
		1: "MODEL_TYPE_EMBEDDING",
		2: "MODEL_TYPE_LLM",
		3: "MODEL_TYPE_CLASSIFIER",
		4: "MODEL_TYPE_NER",
	}
	ModelType_value = map[string]int32{
		"MODEL_TYPE_UNSPECIFIED": 0,
		"MODEL_TYPE_EMBEDDING":   1,
		"MODEL_TYPE_LLM":         2,
		"MODEL_TYPE_CLASSIFIER":  3,
		"MODEL_TYPE_NER":         4,
	}
)

func (x ModelType) Enum() *ModelType {
	p := new(ModelType)
	*p = x
	return p
}

func (x ModelType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ModelType) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proto_ai_v1_ai_proto_enumTypes[1].Descriptor()
}

func (ModelType) Type() protoreflect.EnumType {
	return &file_api_proto_ai_v1_ai_proto_enumTypes[1]
}

func (x ModelType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ModelType.Descriptor instead.
func (ModelType) EnumDescriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{1}
}

// ModelStatus indicates the operational state of a model.
type ModelStatus int32

const (
	// Unknown or unspecified status.
	ModelStatus_MODEL_STATUS_UNSPECIFIED ModelStatus = 0
	// Model is loaded and ready to serve requests.
	ModelStatus_MODEL_STATUS_READY ModelStatus = 1
	// Model is currently loading or warming up.
	ModelStatus_MODEL_STATUS_LOADING ModelStatus = 2
	// Model is unavailable due to an error.
	ModelStatus_MODEL_STATUS_ERROR ModelStatus = 3
	// Model is not loaded and must be loaded before use.
	ModelStatus_MODEL_STATUS_UNLOADED ModelStatus = 4
	// Model is being updated or replaced.
	ModelStatus_MODEL_STATUS_UPDATING ModelStatus = 5
)

// Enum value maps for ModelStatus.
var (
	ModelStatus_name = map[int32]string{
		0: "MODEL_STATUS_UNSPECIFIED",
		1: "MODEL_STATUS_READY",
		2: "MODEL_STATUS_LOADING",
		3: "MODEL_STATUS_ERROR",
		4: "MODEL_STATUS_UNLOADED",
		5: "MODEL_STATUS_UPDATING",
	}
	ModelStatus_value = map[string]int32{
		"MODEL_STATUS_UNSPECIFIED": 0,
		"MODEL_STATUS_READY":       1,
		"MODEL_STATUS_LOADING":     2,
		"MODEL_STATUS_ERROR":       3,
		"MODEL_STATUS_UNLOADED":    4,
		"MODEL_STATUS_UPDATING":    5,
	}
)

func (x ModelStatus) Enum() *ModelStatus {
	p := new(ModelStatus)
	*p = x
	return p
}

func (x ModelStatus) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ModelStatus) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proto_ai_v1_ai_proto_enumTypes[2].Descriptor()
}

func (ModelStatus) Type() protoreflect.EnumType {
	return &file_api_proto_ai_v1_ai_proto_enumTypes[2]
}

func (x ModelStatus) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ModelStatus.Descriptor instead.
func (ModelStatus) EnumDescriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{2}
}

// OptimizationMode determines how the router selects models.
type OptimizationMode int32

const (
	// Default: balanced optimization considering all factors.
	OptimizationMode_OPTIMIZATION_MODE_UNSPECIFIED OptimizationMode = 0
	// Optimize for lowest latency response times.
	OptimizationMode_OPTIMIZATION_MODE_LATENCY OptimizationMode = 1
	// Optimize for highest quality outputs.
	// Prefers larger, more capable models.
	OptimizationMode_OPTIMIZATION_MODE_QUALITY OptimizationMode = 2
	// Optimize for lowest cost per request.
	OptimizationMode_OPTIMIZATION_MODE_COST OptimizationMode = 3
	// Balanced optimization across latency, quality, and cost.
	OptimizationMode_OPTIMIZATION_MODE_BALANCED OptimizationMode = 4
)

// Enum value maps for OptimizationMode.
var (
	OptimizationMode_name = map[int32]string{
		0: "OPTIMIZATION_MODE_UNSPECIFIED",
		1: "OPTIMIZATION_MODE_LATENCY",
		2: "OPTIMIZATION_MODE_QUALITY",
		3: "OPTIMIZATION_MODE_COST",
		4: "OPTIMIZATION_MODE_BALANCED",
	}
	OptimizationMode_value = map[string]int32{
		"OPTIMIZATION_MODE_UNSPECIFIED": 0,
		"OPTIMIZATION_MODE_LATENCY":     1,
		"OPTIMIZATION_MODE_QUALITY":     2,
		"OPTIMIZATION_MODE_COST":        3,
		"OPTIMIZATION_MODE_BALANCED":    4,
	}
)

func (x OptimizationMode) Enum() *OptimizationMode {
	p := new(OptimizationMode)
	*p = x
	return p
}

func (x OptimizationMode) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (OptimizationMode) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proto_ai_v1_ai_proto_enumTypes[3].Descriptor()
}

func (OptimizationMode) Type() protoreflect.EnumType {
	return &file_api_proto_ai_v1_ai_proto_enumTypes[3]
}

func (x OptimizationMode) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use OptimizationMode.Descriptor instead.
func (OptimizationMode) EnumDescriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{3}
}

// AnalysisType specifies the type of analysis to perform.
type AnalysisType int32

const (
	// Unspecified - performs full analysis.
	AnalysisType_ANALYSIS_TYPE_UNSPECIFIED AnalysisType = 0
	// Analyze emotional tone and sentiment.
	AnalysisType_ANALYSIS_TYPE_SENTIMENT AnalysisType = 1
	// Extract key entities (people, places, organizations).
	AnalysisType_ANALYSIS_TYPE_ENTITIES AnalysisType = 2
	// Identify main topics and themes.
	AnalysisType_ANALYSIS_TYPE_TOPICS AnalysisType = 3
	// Extract action items and tasks.
	AnalysisType_ANALYSIS_TYPE_ACTION AnalysisType = 4
	// Comprehensive analysis (all of the above).
	AnalysisType_ANALYSIS_TYPE_FULL AnalysisType = 5
)

// Enum value maps for AnalysisType.
var (
	AnalysisType_name = map[int32]string{
		0: "ANALYSIS_TYPE_UNSPECIFIED",
		1: "ANALYSIS_TYPE_SENTIMENT",
		2: "ANALYSIS_TYPE_ENTITIES",
		3: "ANALYSIS_TYPE_TOPICS",
		4: "ANALYSIS_TYPE_ACTION",
		5: "ANALYSIS_TYPE_FULL",
	}
	AnalysisType_value = map[string]int32{
		"ANALYSIS_TYPE_UNSPECIFIED": 0,
		"ANALYSIS_TYPE_SENTIMENT":   1,
		"ANALYSIS_TYPE_ENTITIES":    2,
		"ANALYSIS_TYPE_TOPICS":      3,
		"ANALYSIS_TYPE_ACTION":      4,
		"ANALYSIS_TYPE_FULL":        5,
	}
)

func (x AnalysisType) Enum() *AnalysisType {
	p := new(AnalysisType)
	*p = x
	return p
}

func (x AnalysisType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (AnalysisType) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proto_ai_v1_ai_proto_enumTypes[4].Descriptor()
}

func (AnalysisType) Type() protoreflect.EnumType {
	return &file_api_proto_ai_v1_ai_proto_enumTypes[4]
}

func (x AnalysisType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use AnalysisType.Descriptor instead.
func (AnalysisType) EnumDescriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{4}
}

// EmbeddingRequest contains the text to generate an embedding for.
type EmbeddingRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The text content to embed.
	// Maximum length depends on the model's context window.
	Text string `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	// The embedding model to use.
	// If not specified, the default model will be used.
	// Examples: "nomic-embed-text", "text-embedding-3-small"
	Model *string `protobuf:"bytes,2,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,3,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,4,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional content ID for Langfuse tag attribute.
	// Expected format: <type:2>-<base62:8> (11 chars), e.g., "em-abc12XYZ".
	ContentId *string `protobuf:"bytes,5,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	// 16-char hex span ID from the root pipeline span.
	PipelineSpanId *string `protobuf:"bytes,6,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *EmbeddingRequest) Reset() {
	*x = EmbeddingRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbeddingRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbeddingRequest) ProtoMessage() {}

func (x *EmbeddingRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbeddingRequest.ProtoReflect.Descriptor instead.
func (*EmbeddingRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{0}
}

func (x *EmbeddingRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *EmbeddingRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *EmbeddingRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *EmbeddingRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *EmbeddingRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *EmbeddingRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// EmbeddingResponse contains the generated vector embedding.
type EmbeddingResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The embedding vector as a list of float values.
	// Length varies by model (typically 768, 1024, or 1536 dimensions).
	Vector []float32 `protobuf:"fixed32,1,rep,packed,name=vector,proto3" json:"vector,omitempty"`
	// The number of dimensions in the embedding vector.
	Dimensions int32 `protobuf:"varint,2,opt,name=dimensions,proto3" json:"dimensions,omitempty"`
	// The model that was actually used to generate the embedding.
	// May differ from requested model if fallback occurred.
	ModelUsed string `protobuf:"bytes,3,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Token count of the input text.
	// Useful for cost tracking and rate limiting.
	TokenCount    *int32 `protobuf:"varint,4,opt,name=token_count,json=tokenCount,proto3,oneof" json:"token_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbeddingResponse) Reset() {
	*x = EmbeddingResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbeddingResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbeddingResponse) ProtoMessage() {}

func (x *EmbeddingResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbeddingResponse.ProtoReflect.Descriptor instead.
func (*EmbeddingResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{1}
}

func (x *EmbeddingResponse) GetVector() []float32 {
	if x != nil {
		return x.Vector
	}
	return nil
}

func (x *EmbeddingResponse) GetDimensions() int32 {
	if x != nil {
		return x.Dimensions
	}
	return 0
}

func (x *EmbeddingResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *EmbeddingResponse) GetTokenCount() int32 {
	if x != nil && x.TokenCount != nil {
		return *x.TokenCount
	}
	return 0
}

// SummaryRequest contains the content to summarize.
type SummaryRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content to summarize.
	// Can be plain text, markdown, or other text formats.
	Content string `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Maximum length of the summary in tokens or characters.
	// Interpretation depends on the model. Default: 150 tokens.
	MaxLength *int32 `protobuf:"varint,2,opt,name=max_length,json=maxLength,proto3,oneof" json:"max_length,omitempty"`
	// The style of summary to generate.
	Style SummaryStyle `protobuf:"varint,3,opt,name=style,proto3,enum=penfold.ai.v1.SummaryStyle" json:"style,omitempty"`
	// The LLM model to use for summarization.
	// If not specified, the default model will be used.
	Model *string `protobuf:"bytes,4,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,5,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Whether to request JSON-structured output from the LLM.
	// When true, the backend will constrain the model to output valid JSON.
	JsonMode *bool `protobuf:"varint,6,opt,name=json_mode,json=jsonMode,proto3,oneof" json:"json_mode,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,7,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional content ID for Langfuse tag attribute.
	// Expected format: <type:2>-<base62:8> (11 chars), e.g., "em-abc12XYZ".
	ContentId *string `protobuf:"bytes,8,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	PipelineSpanId *string `protobuf:"bytes,9,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *SummaryRequest) Reset() {
	*x = SummaryRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummaryRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummaryRequest) ProtoMessage() {}

func (x *SummaryRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummaryRequest.ProtoReflect.Descriptor instead.
func (*SummaryRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{2}
}

func (x *SummaryRequest) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *SummaryRequest) GetMaxLength() int32 {
	if x != nil && x.MaxLength != nil {
		return *x.MaxLength
	}
	return 0
}

func (x *SummaryRequest) GetStyle() SummaryStyle {
	if x != nil {
		return x.Style
	}
	return SummaryStyle_SUMMARY_STYLE_UNSPECIFIED
}

func (x *SummaryRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *SummaryRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *SummaryRequest) GetJsonMode() bool {
	if x != nil && x.JsonMode != nil {
		return *x.JsonMode
	}
	return false
}

func (x *SummaryRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *SummaryRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *SummaryRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// SummaryResponse contains the generated summary and key points.
type SummaryResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The generated summary text.
	Summary string `protobuf:"bytes,1,opt,name=summary,proto3" json:"summary,omitempty"`
	// Key points extracted from the content.
	// Provides quick-reference highlights independent of the summary style.
	KeyPoints []string `protobuf:"bytes,2,rep,name=key_points,json=keyPoints,proto3" json:"key_points,omitempty"`
	// The model that was actually used to generate the summary.
	ModelUsed string `protobuf:"bytes,3,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Token count of the input content.
	InputTokens *int32 `protobuf:"varint,4,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	// Token count of the generated summary.
	OutputTokens *int32 `protobuf:"varint,5,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	// Why the model stopped generating: "stop" (natural), "length" (hit token limit).
	// Empty if not reported by the backend.
	FinishReason  *string `protobuf:"bytes,6,opt,name=finish_reason,json=finishReason,proto3,oneof" json:"finish_reason,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SummaryResponse) Reset() {
	*x = SummaryResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummaryResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummaryResponse) ProtoMessage() {}

func (x *SummaryResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummaryResponse.ProtoReflect.Descriptor instead.
func (*SummaryResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{3}
}

func (x *SummaryResponse) GetSummary() string {
	if x != nil {
		return x.Summary
	}
	return ""
}

func (x *SummaryResponse) GetKeyPoints() []string {
	if x != nil {
		return x.KeyPoints
	}
	return nil
}

func (x *SummaryResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *SummaryResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *SummaryResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

func (x *SummaryResponse) GetFinishReason() string {
	if x != nil && x.FinishReason != nil {
		return *x.FinishReason
	}
	return ""
}

// AssertionRequest contains content from which to extract assertions.
type AssertionRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content to analyze for assertions.
	// Can include emails, documents, meeting notes, etc.
	Content string `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Minimum confidence threshold for returned assertions (0.0 to 1.0).
	// Assertions below this threshold will be filtered out.
	// Default: 0.5
	MinConfidence *float32 `protobuf:"fixed32,2,opt,name=min_confidence,json=minConfidence,proto3,oneof" json:"min_confidence,omitempty"`
	// Maximum number of assertions to return.
	// Default: 20
	MaxAssertions *int32 `protobuf:"varint,3,opt,name=max_assertions,json=maxAssertions,proto3,oneof" json:"max_assertions,omitempty"`
	// The LLM model to use for assertion extraction.
	// If not specified, the default model will be used.
	Model *string `protobuf:"bytes,4,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,5,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,6,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional content ID for Langfuse tag attribute.
	// Expected format: <type:2>-<base62:8> (11 chars), e.g., "em-abc12XYZ".
	ContentId *string `protobuf:"bytes,7,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	PipelineSpanId *string `protobuf:"bytes,8,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *AssertionRequest) Reset() {
	*x = AssertionRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AssertionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AssertionRequest) ProtoMessage() {}

func (x *AssertionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AssertionRequest.ProtoReflect.Descriptor instead.
func (*AssertionRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{4}
}

func (x *AssertionRequest) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *AssertionRequest) GetMinConfidence() float32 {
	if x != nil && x.MinConfidence != nil {
		return *x.MinConfidence
	}
	return 0
}

func (x *AssertionRequest) GetMaxAssertions() int32 {
	if x != nil && x.MaxAssertions != nil {
		return *x.MaxAssertions
	}
	return 0
}

func (x *AssertionRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *AssertionRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *AssertionRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *AssertionRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *AssertionRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// Assertion represents a structured fact or claim extracted from content.
// Modeled as a subject-predicate-object triple.
type Assertion struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The subject of the assertion (e.g., "John", "Project Alpha").
	Subject string `protobuf:"bytes,1,opt,name=subject,proto3" json:"subject,omitempty"`
	// The predicate or relationship (e.g., "works on", "is scheduled for").
	Predicate string `protobuf:"bytes,2,opt,name=predicate,proto3" json:"predicate,omitempty"`
	// The object of the assertion (e.g., "the marketing team", "next Tuesday").
	Object string `protobuf:"bytes,3,opt,name=object,proto3" json:"object,omitempty"`
	// Confidence score for this assertion (0.0 to 1.0).
	// Higher values indicate higher confidence in the extraction accuracy.
	Confidence float32 `protobuf:"fixed32,4,opt,name=confidence,proto3" json:"confidence,omitempty"`
	// The source text span that supports this assertion.
	// Useful for verification and citation.
	SourceText *string `protobuf:"bytes,5,opt,name=source_text,json=sourceText,proto3,oneof" json:"source_text,omitempty"`
	// Category of the assertion (e.g., "temporal", "organizational", "factual").
	Category      *string `protobuf:"bytes,6,opt,name=category,proto3,oneof" json:"category,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Assertion) Reset() {
	*x = Assertion{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Assertion) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Assertion) ProtoMessage() {}

func (x *Assertion) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Assertion.ProtoReflect.Descriptor instead.
func (*Assertion) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{5}
}

func (x *Assertion) GetSubject() string {
	if x != nil {
		return x.Subject
	}
	return ""
}

func (x *Assertion) GetPredicate() string {
	if x != nil {
		return x.Predicate
	}
	return ""
}

func (x *Assertion) GetObject() string {
	if x != nil {
		return x.Object
	}
	return ""
}

func (x *Assertion) GetConfidence() float32 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

func (x *Assertion) GetSourceText() string {
	if x != nil && x.SourceText != nil {
		return *x.SourceText
	}
	return ""
}

func (x *Assertion) GetCategory() string {
	if x != nil && x.Category != nil {
		return *x.Category
	}
	return ""
}

// AssertionResponse contains the extracted assertions.
type AssertionResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of extracted assertions, ordered by confidence (highest first).
	Assertions []*Assertion `protobuf:"bytes,1,rep,name=assertions,proto3" json:"assertions,omitempty"`
	// The model that was actually used for extraction.
	ModelUsed string `protobuf:"bytes,2,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Total number of assertions found before filtering.
	TotalFound int32 `protobuf:"varint,3,opt,name=total_found,json=totalFound,proto3" json:"total_found,omitempty"`
	// Number of assertions filtered out due to confidence threshold.
	FilteredCount int32 `protobuf:"varint,4,opt,name=filtered_count,json=filteredCount,proto3" json:"filtered_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AssertionResponse) Reset() {
	*x = AssertionResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AssertionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AssertionResponse) ProtoMessage() {}

func (x *AssertionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AssertionResponse.ProtoReflect.Descriptor instead.
func (*AssertionResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{6}
}

func (x *AssertionResponse) GetAssertions() []*Assertion {
	if x != nil {
		return x.Assertions
	}
	return nil
}

func (x *AssertionResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *AssertionResponse) GetTotalFound() int32 {
	if x != nil {
		return x.TotalFound
	}
	return 0
}

func (x *AssertionResponse) GetFilteredCount() int32 {
	if x != nil {
		return x.FilteredCount
	}
	return 0
}

// ClassifyContentRequest contains content to classify.
type ClassifyContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content to classify.
	Content string `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Predefined categories to classify into.
	// If empty, the model will use its default taxonomy.
	// Examples: ["work", "personal", "finance", "health"]
	Categories []string `protobuf:"bytes,2,rep,name=categories,proto3" json:"categories,omitempty"`
	// Whether to allow multi-label classification.
	// If false, only the top category will be returned.
	// Default: true
	MultiLabel *bool `protobuf:"varint,3,opt,name=multi_label,json=multiLabel,proto3,oneof" json:"multi_label,omitempty"`
	// Minimum confidence threshold for returned classifications (0.0 to 1.0).
	// Default: 0.3
	MinConfidence *float32 `protobuf:"fixed32,4,opt,name=min_confidence,json=minConfidence,proto3,oneof" json:"min_confidence,omitempty"`
	// The model to use for classification.
	// If not specified, the default model will be used.
	Model *string `protobuf:"bytes,5,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,6,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,7,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional content ID for Langfuse tag attribute.
	// Expected format: <type:2>-<base62:8> (11 chars), e.g., "em-abc12XYZ".
	ContentId *string `protobuf:"bytes,8,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	PipelineSpanId *string `protobuf:"bytes,9,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *ClassifyContentRequest) Reset() {
	*x = ClassifyContentRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ClassifyContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ClassifyContentRequest) ProtoMessage() {}

func (x *ClassifyContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ClassifyContentRequest.ProtoReflect.Descriptor instead.
func (*ClassifyContentRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{7}
}

func (x *ClassifyContentRequest) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *ClassifyContentRequest) GetCategories() []string {
	if x != nil {
		return x.Categories
	}
	return nil
}

func (x *ClassifyContentRequest) GetMultiLabel() bool {
	if x != nil && x.MultiLabel != nil {
		return *x.MultiLabel
	}
	return false
}

func (x *ClassifyContentRequest) GetMinConfidence() float32 {
	if x != nil && x.MinConfidence != nil {
		return *x.MinConfidence
	}
	return 0
}

func (x *ClassifyContentRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *ClassifyContentRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *ClassifyContentRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *ClassifyContentRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *ClassifyContentRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// Classification represents a single category assignment.
type Classification struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The category label.
	Label string `protobuf:"bytes,1,opt,name=label,proto3" json:"label,omitempty"`
	// Confidence score for this classification (0.0 to 1.0).
	Confidence float32 `protobuf:"fixed32,2,opt,name=confidence,proto3" json:"confidence,omitempty"`
	// Optional explanation for why this classification was assigned.
	Explanation   *string `protobuf:"bytes,3,opt,name=explanation,proto3,oneof" json:"explanation,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Classification) Reset() {
	*x = Classification{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Classification) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Classification) ProtoMessage() {}

func (x *Classification) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Classification.ProtoReflect.Descriptor instead.
func (*Classification) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{8}
}

func (x *Classification) GetLabel() string {
	if x != nil {
		return x.Label
	}
	return ""
}

func (x *Classification) GetConfidence() float32 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

func (x *Classification) GetExplanation() string {
	if x != nil && x.Explanation != nil {
		return *x.Explanation
	}
	return ""
}

// ClassifyContentResponse contains the classification results.
type ClassifyContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of classifications, ordered by confidence (highest first).
	Classifications []*Classification `protobuf:"bytes,1,rep,name=classifications,proto3" json:"classifications,omitempty"`
	// The primary classification (highest confidence).
	// Convenience field for single-label use cases.
	Primary *Classification `protobuf:"bytes,2,opt,name=primary,proto3" json:"primary,omitempty"`
	// The model that was actually used for classification.
	ModelUsed     string `protobuf:"bytes,3,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ClassifyContentResponse) Reset() {
	*x = ClassifyContentResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ClassifyContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ClassifyContentResponse) ProtoMessage() {}

func (x *ClassifyContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ClassifyContentResponse.ProtoReflect.Descriptor instead.
func (*ClassifyContentResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{9}
}

func (x *ClassifyContentResponse) GetClassifications() []*Classification {
	if x != nil {
		return x.Classifications
	}
	return nil
}

func (x *ClassifyContentResponse) GetPrimary() *Classification {
	if x != nil {
		return x.Primary
	}
	return nil
}

func (x *ClassifyContentResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

// TriageContentRequest contains content to triage (category + importance).
type TriageContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content to triage (first ~500 characters recommended).
	Content string `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Optional email subject for additional context.
	Subject *string `protobuf:"bytes,2,opt,name=subject,proto3,oneof" json:"subject,omitempty"`
	// Optional sender for additional context.
	Sender *string `protobuf:"bytes,3,opt,name=sender,proto3,oneof" json:"sender,omitempty"`
	// The model to use for triage.
	// If not specified, the default SLM will be used.
	Model *string `protobuf:"bytes,4,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,5,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Optional source ID for pipeline_runs provenance tracking.
	SourceId *int64 `protobuf:"varint,6,opt,name=source_id,json=sourceId,proto3,oneof" json:"source_id,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,7,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional content ID for Langfuse tag attribute.
	// Expected format: <type:2>-<base62:8> (11 chars), e.g., "em-abc12XYZ".
	ContentId *string `protobuf:"bytes,8,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	PipelineSpanId *string `protobuf:"bytes,9,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *TriageContentRequest) Reset() {
	*x = TriageContentRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TriageContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TriageContentRequest) ProtoMessage() {}

func (x *TriageContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TriageContentRequest.ProtoReflect.Descriptor instead.
func (*TriageContentRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{10}
}

func (x *TriageContentRequest) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *TriageContentRequest) GetSubject() string {
	if x != nil && x.Subject != nil {
		return *x.Subject
	}
	return ""
}

func (x *TriageContentRequest) GetSender() string {
	if x != nil && x.Sender != nil {
		return *x.Sender
	}
	return ""
}

func (x *TriageContentRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *TriageContentRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *TriageContentRequest) GetSourceId() int64 {
	if x != nil && x.SourceId != nil {
		return *x.SourceId
	}
	return 0
}

func (x *TriageContentRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *TriageContentRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *TriageContentRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// TriageContentResponse contains the triage classification results.
type TriageContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Category classification.
	// One of: PROJECT_UPDATE, CUSTOMER, RISK_ISSUE, ACTION_REQUEST,
	// DECISION, INTERNAL_COMMS, PERSONAL, OTHER
	Category string `protobuf:"bytes,1,opt,name=category,proto3" json:"category,omitempty"`
	// Importance level.
	// One of: HIGH, MEDIUM, LOW
	Importance string `protobuf:"bytes,2,opt,name=importance,proto3" json:"importance,omitempty"`
	// Brief explanation for the classification.
	Reason string `protobuf:"bytes,3,opt,name=reason,proto3" json:"reason,omitempty"`
	// The model that was actually used for triage.
	ModelUsed string `protobuf:"bytes,4,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Number of input tokens processed.
	InputTokens *int32 `protobuf:"varint,5,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	// Number of output tokens generated.
	OutputTokens *int32 `protobuf:"varint,6,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	// Number of retries needed to get a valid response.
	Retries int32 `protobuf:"varint,7,opt,name=retries,proto3" json:"retries,omitempty"`
	// Content contribution level (how much NEW information the message contributes).
	// One of: HIGH, MEDIUM, LOW, NONE
	// Used to gate expensive pipeline stages.
	ContentContribution *string `protobuf:"bytes,8,opt,name=content_contribution,json=contentContribution,proto3,oneof" json:"content_contribution,omitempty"`
	// Brief explanation for the contribution assessment.
	ContributionReason *string `protobuf:"bytes,9,opt,name=contribution_reason,json=contributionReason,proto3,oneof" json:"contribution_reason,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *TriageContentResponse) Reset() {
	*x = TriageContentResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TriageContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TriageContentResponse) ProtoMessage() {}

func (x *TriageContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TriageContentResponse.ProtoReflect.Descriptor instead.
func (*TriageContentResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{11}
}

func (x *TriageContentResponse) GetCategory() string {
	if x != nil {
		return x.Category
	}
	return ""
}

func (x *TriageContentResponse) GetImportance() string {
	if x != nil {
		return x.Importance
	}
	return ""
}

func (x *TriageContentResponse) GetReason() string {
	if x != nil {
		return x.Reason
	}
	return ""
}

func (x *TriageContentResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *TriageContentResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *TriageContentResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

func (x *TriageContentResponse) GetRetries() int32 {
	if x != nil {
		return x.Retries
	}
	return 0
}

func (x *TriageContentResponse) GetContentContribution() string {
	if x != nil && x.ContentContribution != nil {
		return *x.ContentContribution
	}
	return ""
}

func (x *TriageContentResponse) GetContributionReason() string {
	if x != nil && x.ContributionReason != nil {
		return *x.ContributionReason
	}
	return ""
}

// ExtractEntitiesRequest contains content for two-pass entity extraction.
type ExtractEntitiesRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content to extract entities from.
	Content string `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Triage category from Stage 1 (used for quality gate).
	// If RISK_ISSUE and extraction finds 0 risks, re-runs with focused prompt.
	TriageCategory *string `protobuf:"bytes,2,opt,name=triage_category,json=triageCategory,proto3,oneof" json:"triage_category,omitempty"`
	// The model to use for extraction.
	Model *string `protobuf:"bytes,3,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Optional tenant identifier.
	TenantId *string `protobuf:"bytes,4,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Optional source ID for provenance tracking.
	SourceId *int64 `protobuf:"varint,5,opt,name=source_id,json=sourceId,proto3,oneof" json:"source_id,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,6,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional content ID for Langfuse tag attribute.
	// Expected format: <type:2>-<base62:8> (11 chars), e.g., "em-abc12XYZ".
	ContentId *string `protobuf:"bytes,7,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	PipelineSpanId *string `protobuf:"bytes,8,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *ExtractEntitiesRequest) Reset() {
	*x = ExtractEntitiesRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExtractEntitiesRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExtractEntitiesRequest) ProtoMessage() {}

func (x *ExtractEntitiesRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExtractEntitiesRequest.ProtoReflect.Descriptor instead.
func (*ExtractEntitiesRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{12}
}

func (x *ExtractEntitiesRequest) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *ExtractEntitiesRequest) GetTriageCategory() string {
	if x != nil && x.TriageCategory != nil {
		return *x.TriageCategory
	}
	return ""
}

func (x *ExtractEntitiesRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *ExtractEntitiesRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *ExtractEntitiesRequest) GetSourceId() int64 {
	if x != nil && x.SourceId != nil {
		return *x.SourceId
	}
	return 0
}

func (x *ExtractEntitiesRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *ExtractEntitiesRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *ExtractEntitiesRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// PersonEntity represents a person extracted from content.
type PersonEntity struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Name          string                 `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	Role          string                 `protobuf:"bytes,2,opt,name=role,proto3" json:"role,omitempty"` // Role/title if stated
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PersonEntity) Reset() {
	*x = PersonEntity{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PersonEntity) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PersonEntity) ProtoMessage() {}

func (x *PersonEntity) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PersonEntity.ProtoReflect.Descriptor instead.
func (*PersonEntity) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{13}
}

func (x *PersonEntity) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *PersonEntity) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

// DateEntity represents a date or deadline extracted from content.
type DateEntity struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Date          string                 `protobuf:"bytes,1,opt,name=date,proto3" json:"date,omitempty"`
	Context       string                 `protobuf:"bytes,2,opt,name=context,proto3" json:"context,omitempty"` // What the date relates to
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DateEntity) Reset() {
	*x = DateEntity{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DateEntity) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DateEntity) ProtoMessage() {}

func (x *DateEntity) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DateEntity.ProtoReflect.Descriptor instead.
func (*DateEntity) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{14}
}

func (x *DateEntity) GetDate() string {
	if x != nil {
		return x.Date
	}
	return ""
}

func (x *DateEntity) GetContext() string {
	if x != nil {
		return x.Context
	}
	return ""
}

// ActionItemEntity represents an explicit action item.
type ActionItemEntity struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Assignee      string                 `protobuf:"bytes,1,opt,name=assignee,proto3" json:"assignee,omitempty"`
	Action        string                 `protobuf:"bytes,2,opt,name=action,proto3" json:"action,omitempty"`
	Due           string                 `protobuf:"bytes,3,opt,name=due,proto3" json:"due,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ActionItemEntity) Reset() {
	*x = ActionItemEntity{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ActionItemEntity) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ActionItemEntity) ProtoMessage() {}

func (x *ActionItemEntity) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ActionItemEntity.ProtoReflect.Descriptor instead.
func (*ActionItemEntity) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{15}
}

func (x *ActionItemEntity) GetAssignee() string {
	if x != nil {
		return x.Assignee
	}
	return ""
}

func (x *ActionItemEntity) GetAction() string {
	if x != nil {
		return x.Action
	}
	return ""
}

func (x *ActionItemEntity) GetDue() string {
	if x != nil {
		return x.Due
	}
	return ""
}

// RiskEntity represents a risk from the quality gate re-run.
// Used when triage=RISK_ISSUE triggers focused extraction.
type RiskEntity struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Description   string                 `protobuf:"bytes,1,opt,name=description,proto3" json:"description,omitempty"`
	SeverityHint  string                 `protobuf:"bytes,2,opt,name=severity_hint,json=severityHint,proto3" json:"severity_hint,omitempty"`
	OwnerHint     string                 `protobuf:"bytes,3,opt,name=owner_hint,json=ownerHint,proto3" json:"owner_hint,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RiskEntity) Reset() {
	*x = RiskEntity{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RiskEntity) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RiskEntity) ProtoMessage() {}

func (x *RiskEntity) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RiskEntity.ProtoReflect.Descriptor instead.
func (*RiskEntity) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{16}
}

func (x *RiskEntity) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *RiskEntity) GetSeverityHint() string {
	if x != nil {
		return x.SeverityHint
	}
	return ""
}

func (x *RiskEntity) GetOwnerHint() string {
	if x != nil {
		return x.OwnerHint
	}
	return ""
}

// ExtractEntitiesResponse contains the merged extraction results.
type ExtractEntitiesResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Stage 2a: NER results
	People        []*PersonEntity `protobuf:"bytes,1,rep,name=people,proto3" json:"people,omitempty"`
	Dates         []*DateEntity   `protobuf:"bytes,2,rep,name=dates,proto3" json:"dates,omitempty"`
	Projects      []string        `protobuf:"bytes,3,rep,name=projects,proto3" json:"projects,omitempty"`
	Organisations []string        `protobuf:"bytes,4,rep,name=organisations,proto3" json:"organisations,omitempty"`
	// Stage 2b: Semantic results
	ActionItems []*ActionItemEntity `protobuf:"bytes,5,rep,name=action_items,json=actionItems,proto3" json:"action_items,omitempty"`
	Decisions   []string            `protobuf:"bytes,6,rep,name=decisions,proto3" json:"decisions,omitempty"`
	Risks       []string            `protobuf:"bytes,7,rep,name=risks,proto3" json:"risks,omitempty"`
	// Quality gate results (only populated if re-run triggered)
	DetailedRisks        []*RiskEntity `protobuf:"bytes,8,rep,name=detailed_risks,json=detailedRisks,proto3" json:"detailed_risks,omitempty"`
	QualityGateTriggered bool          `protobuf:"varint,9,opt,name=quality_gate_triggered,json=qualityGateTriggered,proto3" json:"quality_gate_triggered,omitempty"`
	// Metadata
	ModelUsed     string `protobuf:"bytes,10,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	InputTokens   *int32 `protobuf:"varint,11,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	OutputTokens  *int32 `protobuf:"varint,12,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	Retries       int32  `protobuf:"varint,13,opt,name=retries,proto3" json:"retries,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ExtractEntitiesResponse) Reset() {
	*x = ExtractEntitiesResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExtractEntitiesResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExtractEntitiesResponse) ProtoMessage() {}

func (x *ExtractEntitiesResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExtractEntitiesResponse.ProtoReflect.Descriptor instead.
func (*ExtractEntitiesResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{17}
}

func (x *ExtractEntitiesResponse) GetPeople() []*PersonEntity {
	if x != nil {
		return x.People
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetDates() []*DateEntity {
	if x != nil {
		return x.Dates
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetProjects() []string {
	if x != nil {
		return x.Projects
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetOrganisations() []string {
	if x != nil {
		return x.Organisations
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetActionItems() []*ActionItemEntity {
	if x != nil {
		return x.ActionItems
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetDecisions() []string {
	if x != nil {
		return x.Decisions
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetRisks() []string {
	if x != nil {
		return x.Risks
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetDetailedRisks() []*RiskEntity {
	if x != nil {
		return x.DetailedRisks
	}
	return nil
}

func (x *ExtractEntitiesResponse) GetQualityGateTriggered() bool {
	if x != nil {
		return x.QualityGateTriggered
	}
	return false
}

func (x *ExtractEntitiesResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *ExtractEntitiesResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *ExtractEntitiesResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

func (x *ExtractEntitiesResponse) GetRetries() int32 {
	if x != nil {
		return x.Retries
	}
	return 0
}

// DeepAnalyzeRequest contains pre-processed input for Stage 4 deep analysis.
type DeepAnalyzeRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Clean text from Stage 0
	Content string `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Stage 2 NER extraction results (verified entities)
	VerifiedPeople        []*PersonEntity `protobuf:"bytes,2,rep,name=verified_people,json=verifiedPeople,proto3" json:"verified_people,omitempty"`
	VerifiedDates         []*DateEntity   `protobuf:"bytes,3,rep,name=verified_dates,json=verifiedDates,proto3" json:"verified_dates,omitempty"`
	VerifiedProjects      []string        `protobuf:"bytes,4,rep,name=verified_projects,json=verifiedProjects,proto3" json:"verified_projects,omitempty"`
	VerifiedOrganisations []string        `protobuf:"bytes,5,rep,name=verified_organisations,json=verifiedOrganisations,proto3" json:"verified_organisations,omitempty"`
	// Stage 2 semantic extraction results (preliminary, needs verification)
	PreliminaryActionItems []*ActionItemEntity `protobuf:"bytes,6,rep,name=preliminary_action_items,json=preliminaryActionItems,proto3" json:"preliminary_action_items,omitempty"`
	PreliminaryDecisions   []string            `protobuf:"bytes,7,rep,name=preliminary_decisions,json=preliminaryDecisions,proto3" json:"preliminary_decisions,omitempty"`
	PreliminaryRisks       []string            `protobuf:"bytes,8,rep,name=preliminary_risks,json=preliminaryRisks,proto3" json:"preliminary_risks,omitempty"`
	// Stage 3 resolved context (background knowledge from KB)
	BackgroundContext string `protobuf:"bytes,9,opt,name=background_context,json=backgroundContext,proto3" json:"background_context,omitempty"`
	// Triage metadata for model selection
	TriageCategory   string `protobuf:"bytes,10,opt,name=triage_category,json=triageCategory,proto3" json:"triage_category,omitempty"`       // e.g., RISK_ISSUE, PROJECT_UPDATE
	TriageImportance string `protobuf:"bytes,11,opt,name=triage_importance,json=triageImportance,proto3" json:"triage_importance,omitempty"` // HIGH, MEDIUM, LOW
	// Metadata
	Model     *string `protobuf:"bytes,12,opt,name=model,proto3,oneof" json:"model,omitempty"`
	TenantId  *string `protobuf:"bytes,13,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	SourceId  *int64  `protobuf:"varint,14,opt,name=source_id,json=sourceId,proto3,oneof" json:"source_id,omitempty"`
	ContentId *string `protobuf:"bytes,15,opt,name=content_id,json=contentId,proto3,oneof" json:"content_id,omitempty"`
	// Optional pipeline trace ID for grouping related operations.
	// When set, this span will be a child of the pipeline trace.
	PipelineTraceId *string `protobuf:"bytes,16,opt,name=pipeline_trace_id,json=pipelineTraceId,proto3,oneof" json:"pipeline_trace_id,omitempty"`
	// Optional pipeline span ID for proper parent-child hierarchy.
	PipelineSpanId *string `protobuf:"bytes,17,opt,name=pipeline_span_id,json=pipelineSpanId,proto3,oneof" json:"pipeline_span_id,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *DeepAnalyzeRequest) Reset() {
	*x = DeepAnalyzeRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeepAnalyzeRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeepAnalyzeRequest) ProtoMessage() {}

func (x *DeepAnalyzeRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeepAnalyzeRequest.ProtoReflect.Descriptor instead.
func (*DeepAnalyzeRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{18}
}

func (x *DeepAnalyzeRequest) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetVerifiedPeople() []*PersonEntity {
	if x != nil {
		return x.VerifiedPeople
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetVerifiedDates() []*DateEntity {
	if x != nil {
		return x.VerifiedDates
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetVerifiedProjects() []string {
	if x != nil {
		return x.VerifiedProjects
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetVerifiedOrganisations() []string {
	if x != nil {
		return x.VerifiedOrganisations
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetPreliminaryActionItems() []*ActionItemEntity {
	if x != nil {
		return x.PreliminaryActionItems
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetPreliminaryDecisions() []string {
	if x != nil {
		return x.PreliminaryDecisions
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetPreliminaryRisks() []string {
	if x != nil {
		return x.PreliminaryRisks
	}
	return nil
}

func (x *DeepAnalyzeRequest) GetBackgroundContext() string {
	if x != nil {
		return x.BackgroundContext
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetTriageCategory() string {
	if x != nil {
		return x.TriageCategory
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetTriageImportance() string {
	if x != nil {
		return x.TriageImportance
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetSourceId() int64 {
	if x != nil && x.SourceId != nil {
		return *x.SourceId
	}
	return 0
}

func (x *DeepAnalyzeRequest) GetContentId() string {
	if x != nil && x.ContentId != nil {
		return *x.ContentId
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetPipelineTraceId() string {
	if x != nil && x.PipelineTraceId != nil {
		return *x.PipelineTraceId
	}
	return ""
}

func (x *DeepAnalyzeRequest) GetPipelineSpanId() string {
	if x != nil && x.PipelineSpanId != nil {
		return *x.PipelineSpanId
	}
	return ""
}

// DeepAnalyzeResponse contains the Stage 4 analysis results.
type DeepAnalyzeResponse struct {
	state               protoimpl.MessageState `protogen:"open.v1"`
	Summary             string                 `protobuf:"bytes,1,opt,name=summary,proto3" json:"summary,omitempty"`
	Sentiment           *DeepSentiment         `protobuf:"bytes,2,opt,name=sentiment,proto3" json:"sentiment,omitempty"`
	TopicMappings       []*TopicMapping        `protobuf:"bytes,3,rep,name=topic_mappings,json=topicMappings,proto3" json:"topic_mappings,omitempty"`
	VerifiedActionItems []*VerifiedActionItem  `protobuf:"bytes,4,rep,name=verified_action_items,json=verifiedActionItems,proto3" json:"verified_action_items,omitempty"`
	VerifiedDecisions   []*VerifiedDecision    `protobuf:"bytes,5,rep,name=verified_decisions,json=verifiedDecisions,proto3" json:"verified_decisions,omitempty"`
	RiskReferences      []*RiskReference       `protobuf:"bytes,6,rep,name=risk_references,json=riskReferences,proto3" json:"risk_references,omitempty"`
	StrategicInsights   []string               `protobuf:"bytes,7,rep,name=strategic_insights,json=strategicInsights,proto3" json:"strategic_insights,omitempty"`
	ImplicitActionItems []*ImplicitActionItem  `protobuf:"bytes,8,rep,name=implicit_action_items,json=implicitActionItems,proto3" json:"implicit_action_items,omitempty"`
	ModelUsed           string                 `protobuf:"bytes,9,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	InputTokens         *int32                 `protobuf:"varint,10,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	OutputTokens        *int32                 `protobuf:"varint,11,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	unknownFields       protoimpl.UnknownFields
	sizeCache           protoimpl.SizeCache
}

func (x *DeepAnalyzeResponse) Reset() {
	*x = DeepAnalyzeResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeepAnalyzeResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeepAnalyzeResponse) ProtoMessage() {}

func (x *DeepAnalyzeResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeepAnalyzeResponse.ProtoReflect.Descriptor instead.
func (*DeepAnalyzeResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{19}
}

func (x *DeepAnalyzeResponse) GetSummary() string {
	if x != nil {
		return x.Summary
	}
	return ""
}

func (x *DeepAnalyzeResponse) GetSentiment() *DeepSentiment {
	if x != nil {
		return x.Sentiment
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetTopicMappings() []*TopicMapping {
	if x != nil {
		return x.TopicMappings
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetVerifiedActionItems() []*VerifiedActionItem {
	if x != nil {
		return x.VerifiedActionItems
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetVerifiedDecisions() []*VerifiedDecision {
	if x != nil {
		return x.VerifiedDecisions
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetRiskReferences() []*RiskReference {
	if x != nil {
		return x.RiskReferences
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetStrategicInsights() []string {
	if x != nil {
		return x.StrategicInsights
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetImplicitActionItems() []*ImplicitActionItem {
	if x != nil {
		return x.ImplicitActionItems
	}
	return nil
}

func (x *DeepAnalyzeResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *DeepAnalyzeResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *DeepAnalyzeResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

// DeepSentiment contains business-context-aware sentiment analysis.
type DeepSentiment struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Score         float32                `protobuf:"fixed32,1,opt,name=score,proto3" json:"score,omitempty"` // -1.0 to 1.0
	Label         string                 `protobuf:"bytes,2,opt,name=label,proto3" json:"label,omitempty"`   // positive, negative, neutral, mixed
	Confidence    float32                `protobuf:"fixed32,3,opt,name=confidence,proto3" json:"confidence,omitempty"`
	Indicators    []string               `protobuf:"bytes,4,rep,name=indicators,proto3" json:"indicators,omitempty"`
	Explanation   string                 `protobuf:"bytes,5,opt,name=explanation,proto3" json:"explanation,omitempty"` // Business context explanation
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DeepSentiment) Reset() {
	*x = DeepSentiment{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeepSentiment) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeepSentiment) ProtoMessage() {}

func (x *DeepSentiment) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeepSentiment.ProtoReflect.Descriptor instead.
func (*DeepSentiment) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{20}
}

func (x *DeepSentiment) GetScore() float32 {
	if x != nil {
		return x.Score
	}
	return 0
}

func (x *DeepSentiment) GetLabel() string {
	if x != nil {
		return x.Label
	}
	return ""
}

func (x *DeepSentiment) GetConfidence() float32 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

func (x *DeepSentiment) GetIndicators() []string {
	if x != nil {
		return x.Indicators
	}
	return nil
}

func (x *DeepSentiment) GetExplanation() string {
	if x != nil {
		return x.Explanation
	}
	return ""
}

// TopicMapping connects content to known projects/products.
type TopicMapping struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	Topic          string                 `protobuf:"bytes,1,opt,name=topic,proto3" json:"topic,omitempty"`
	RelatedProject string                 `protobuf:"bytes,2,opt,name=related_project,json=relatedProject,proto3" json:"related_project,omitempty"` // Resolved product/project name
	Relationship   string                 `protobuf:"bytes,3,opt,name=relationship,proto3" json:"relationship,omitempty"`                           // How content relates to this topic
	Confidence     float32                `protobuf:"fixed32,4,opt,name=confidence,proto3" json:"confidence,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *TopicMapping) Reset() {
	*x = TopicMapping{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TopicMapping) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TopicMapping) ProtoMessage() {}

func (x *TopicMapping) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TopicMapping.ProtoReflect.Descriptor instead.
func (*TopicMapping) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{21}
}

func (x *TopicMapping) GetTopic() string {
	if x != nil {
		return x.Topic
	}
	return ""
}

func (x *TopicMapping) GetRelatedProject() string {
	if x != nil {
		return x.RelatedProject
	}
	return ""
}

func (x *TopicMapping) GetRelationship() string {
	if x != nil {
		return x.Relationship
	}
	return ""
}

func (x *TopicMapping) GetConfidence() float32 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

// VerifiedActionItem represents an action item verified/refined by LLM.
type VerifiedActionItem struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	Description    string                 `protobuf:"bytes,1,opt,name=description,proto3" json:"description,omitempty"`
	Assignee       string                 `protobuf:"bytes,2,opt,name=assignee,proto3" json:"assignee,omitempty"`
	Due            string                 `protobuf:"bytes,3,opt,name=due,proto3" json:"due,omitempty"`
	Priority       string                 `protobuf:"bytes,4,opt,name=priority,proto3" json:"priority,omitempty"`                                   // high, medium, low
	ContextExcerpt string                 `protobuf:"bytes,5,opt,name=context_excerpt,json=contextExcerpt,proto3" json:"context_excerpt,omitempty"` // Direct quote from content (REQUIRED)
	Status         string                 `protobuf:"bytes,6,opt,name=status,proto3" json:"status,omitempty"`                                       // confirmed, refined, removed, new
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *VerifiedActionItem) Reset() {
	*x = VerifiedActionItem{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[22]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *VerifiedActionItem) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*VerifiedActionItem) ProtoMessage() {}

func (x *VerifiedActionItem) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[22]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use VerifiedActionItem.ProtoReflect.Descriptor instead.
func (*VerifiedActionItem) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{22}
}

func (x *VerifiedActionItem) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *VerifiedActionItem) GetAssignee() string {
	if x != nil {
		return x.Assignee
	}
	return ""
}

func (x *VerifiedActionItem) GetDue() string {
	if x != nil {
		return x.Due
	}
	return ""
}

func (x *VerifiedActionItem) GetPriority() string {
	if x != nil {
		return x.Priority
	}
	return ""
}

func (x *VerifiedActionItem) GetContextExcerpt() string {
	if x != nil {
		return x.ContextExcerpt
	}
	return ""
}

func (x *VerifiedActionItem) GetStatus() string {
	if x != nil {
		return x.Status
	}
	return ""
}

// VerifiedDecision represents a decision verified/refined by LLM.
type VerifiedDecision struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	Description    string                 `protobuf:"bytes,1,opt,name=description,proto3" json:"description,omitempty"`
	ContextExcerpt string                 `protobuf:"bytes,2,opt,name=context_excerpt,json=contextExcerpt,proto3" json:"context_excerpt,omitempty"` // Direct quote from content (REQUIRED)
	Status         string                 `protobuf:"bytes,3,opt,name=status,proto3" json:"status,omitempty"`                                       // confirmed, refined, removed, new
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *VerifiedDecision) Reset() {
	*x = VerifiedDecision{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[23]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *VerifiedDecision) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*VerifiedDecision) ProtoMessage() {}

func (x *VerifiedDecision) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[23]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use VerifiedDecision.ProtoReflect.Descriptor instead.
func (*VerifiedDecision) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{23}
}

func (x *VerifiedDecision) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *VerifiedDecision) GetContextExcerpt() string {
	if x != nil {
		return x.ContextExcerpt
	}
	return ""
}

func (x *VerifiedDecision) GetStatus() string {
	if x != nil {
		return x.Status
	}
	return ""
}

// RiskReference connects content to existing or new risks.
type RiskReference struct {
	state           protoimpl.MessageState `protogen:"open.v1"`
	RootId          *int64                 `protobuf:"varint,1,opt,name=root_id,json=rootId,proto3,oneof" json:"root_id,omitempty"` // If matching existing risk
	Description     string                 `protobuf:"bytes,2,opt,name=description,proto3" json:"description,omitempty"`
	LifecycleChange *string                `protobuf:"bytes,3,opt,name=lifecycle_change,json=lifecycleChange,proto3,oneof" json:"lifecycle_change,omitempty"` // escalated, de_escalated, assigned, decided, deferred, resolved
	Significance    string                 `protobuf:"bytes,4,opt,name=significance,proto3" json:"significance,omitempty"`                                    // primary, secondary, passing
	ContextExcerpt  string                 `protobuf:"bytes,5,opt,name=context_excerpt,json=contextExcerpt,proto3" json:"context_excerpt,omitempty"`          // Direct quote from content (REQUIRED)
	SeverityChange  *string                `protobuf:"bytes,6,opt,name=severity_change,json=severityChange,proto3,oneof" json:"severity_change,omitempty"`
	OwnerChange     *string                `protobuf:"bytes,7,opt,name=owner_change,json=ownerChange,proto3,oneof" json:"owner_change,omitempty"`
	IsNew           bool                   `protobuf:"varint,8,opt,name=is_new,json=isNew,proto3" json:"is_new,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *RiskReference) Reset() {
	*x = RiskReference{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RiskReference) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RiskReference) ProtoMessage() {}

func (x *RiskReference) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RiskReference.ProtoReflect.Descriptor instead.
func (*RiskReference) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{24}
}

func (x *RiskReference) GetRootId() int64 {
	if x != nil && x.RootId != nil {
		return *x.RootId
	}
	return 0
}

func (x *RiskReference) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *RiskReference) GetLifecycleChange() string {
	if x != nil && x.LifecycleChange != nil {
		return *x.LifecycleChange
	}
	return ""
}

func (x *RiskReference) GetSignificance() string {
	if x != nil {
		return x.Significance
	}
	return ""
}

func (x *RiskReference) GetContextExcerpt() string {
	if x != nil {
		return x.ContextExcerpt
	}
	return ""
}

func (x *RiskReference) GetSeverityChange() string {
	if x != nil && x.SeverityChange != nil {
		return *x.SeverityChange
	}
	return ""
}

func (x *RiskReference) GetOwnerChange() string {
	if x != nil && x.OwnerChange != nil {
		return *x.OwnerChange
	}
	return ""
}

func (x *RiskReference) GetIsNew() bool {
	if x != nil {
		return x.IsNew
	}
	return false
}

// ImplicitActionItem represents an inferred action not explicitly stated.
type ImplicitActionItem struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	Description    string                 `protobuf:"bytes,1,opt,name=description,proto3" json:"description,omitempty"`
	Reasoning      string                 `protobuf:"bytes,2,opt,name=reasoning,proto3" json:"reasoning,omitempty"`                                 // Why this is inferred
	ContextExcerpt string                 `protobuf:"bytes,3,opt,name=context_excerpt,json=contextExcerpt,proto3" json:"context_excerpt,omitempty"` // Supporting quote from content (REQUIRED)
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *ImplicitActionItem) Reset() {
	*x = ImplicitActionItem{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[25]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ImplicitActionItem) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ImplicitActionItem) ProtoMessage() {}

func (x *ImplicitActionItem) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[25]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ImplicitActionItem.ProtoReflect.Descriptor instead.
func (*ImplicitActionItem) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{25}
}

func (x *ImplicitActionItem) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *ImplicitActionItem) GetReasoning() string {
	if x != nil {
		return x.Reasoning
	}
	return ""
}

func (x *ImplicitActionItem) GetContextExcerpt() string {
	if x != nil {
		return x.ContextExcerpt
	}
	return ""
}

// ModelInfo contains information about an available AI model.
type ModelInfo struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Unique identifier for the model in the registry.
	// Generated by the system when registering a new model.
	Id string `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// Human-readable name for the model.
	// Examples: "GPT-4 Turbo", "Local Nomic Embeddings"
	Name string `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`
	// The type of model.
	Type ModelType `protobuf:"varint,3,opt,name=type,proto3,enum=penfold.ai.v1.ModelType" json:"type,omitempty"`
	// Current operational status of the model.
	Status ModelStatus `protobuf:"varint,4,opt,name=status,proto3,enum=penfold.ai.v1.ModelStatus" json:"status,omitempty"`
	// List of capabilities this model supports.
	// Examples: ["embedding", "chat", "summarization", "extraction"]
	Capabilities []string `protobuf:"bytes,5,rep,name=capabilities,proto3" json:"capabilities,omitempty"`
	// Maximum context length in tokens.
	// Relevant for LLMs and embedding models.
	MaxContextLength *int32 `protobuf:"varint,6,opt,name=max_context_length,json=maxContextLength,proto3,oneof" json:"max_context_length,omitempty"`
	// Embedding dimensions (for embedding models only).
	EmbeddingDimensions *int32 `protobuf:"varint,7,opt,name=embedding_dimensions,json=embeddingDimensions,proto3,oneof" json:"embedding_dimensions,omitempty"`
	// Whether this is a local model (e.g., Ollama) or cloud API.
	IsLocal bool `protobuf:"varint,8,opt,name=is_local,json=isLocal,proto3" json:"is_local,omitempty"`
	// Provider name (e.g., "ollama", "openai", "google", "anthropic").
	Provider string `protobuf:"bytes,9,opt,name=provider,proto3" json:"provider,omitempty"`
	// The provider's model identifier.
	// Examples: "gpt-4-turbo", "nomic-embed-text", "gemini-1.5-pro"
	ModelName string `protobuf:"bytes,10,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// Model version or variant.
	Version *string `protobuf:"bytes,11,opt,name=version,proto3,oneof" json:"version,omitempty"`
	// Error message if status is MODEL_STATUS_ERROR.
	ErrorMessage *string `protobuf:"bytes,12,opt,name=error_message,json=errorMessage,proto3,oneof" json:"error_message,omitempty"`
	// Average latency in milliseconds for recent requests.
	AvgLatencyMs *float64 `protobuf:"fixed64,13,opt,name=avg_latency_ms,json=avgLatencyMs,proto3,oneof" json:"avg_latency_ms,omitempty"`
	// Number of requests processed in the last hour.
	RequestsLastHour *int64 `protobuf:"varint,14,opt,name=requests_last_hour,json=requestsLastHour,proto3,oneof" json:"requests_last_hour,omitempty"`
	// API endpoint URL for the model.
	// For local models, this might be "http://localhost:11434".
	// For cloud APIs, this is typically the provider's API URL.
	Endpoint *string `protobuf:"bytes,15,opt,name=endpoint,proto3,oneof" json:"endpoint,omitempty"`
	// Cost per 1,000 input tokens in USD.
	// Used for cost optimization in routing decisions.
	InputCostPer_1K *float64 `protobuf:"fixed64,16,opt,name=input_cost_per_1k,json=inputCostPer1k,proto3,oneof" json:"input_cost_per_1k,omitempty"`
	// Cost per 1,000 output tokens in USD.
	// Used for cost optimization in routing decisions.
	OutputCostPer_1K *float64 `protobuf:"fixed64,17,opt,name=output_cost_per_1k,json=outputCostPer1k,proto3,oneof" json:"output_cost_per_1k,omitempty"`
	// Priority for model selection (higher = preferred).
	// Used when multiple models have similar capabilities.
	// Default: 0
	Priority int32 `protobuf:"varint,18,opt,name=priority,proto3" json:"priority,omitempty"`
	// Whether this model is enabled for use.
	// Disabled models are not selected by routing but remain in registry.
	IsEnabled     bool `protobuf:"varint,19,opt,name=is_enabled,json=isEnabled,proto3" json:"is_enabled,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInfo) Reset() {
	*x = ModelInfo{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[26]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInfo) ProtoMessage() {}

func (x *ModelInfo) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[26]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInfo.ProtoReflect.Descriptor instead.
func (*ModelInfo) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{26}
}

func (x *ModelInfo) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInfo) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInfo) GetType() ModelType {
	if x != nil {
		return x.Type
	}
	return ModelType_MODEL_TYPE_UNSPECIFIED
}

func (x *ModelInfo) GetStatus() ModelStatus {
	if x != nil {
		return x.Status
	}
	return ModelStatus_MODEL_STATUS_UNSPECIFIED
}

func (x *ModelInfo) GetCapabilities() []string {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

func (x *ModelInfo) GetMaxContextLength() int32 {
	if x != nil && x.MaxContextLength != nil {
		return *x.MaxContextLength
	}
	return 0
}

func (x *ModelInfo) GetEmbeddingDimensions() int32 {
	if x != nil && x.EmbeddingDimensions != nil {
		return *x.EmbeddingDimensions
	}
	return 0
}

func (x *ModelInfo) GetIsLocal() bool {
	if x != nil {
		return x.IsLocal
	}
	return false
}

func (x *ModelInfo) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

func (x *ModelInfo) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *ModelInfo) GetVersion() string {
	if x != nil && x.Version != nil {
		return *x.Version
	}
	return ""
}

func (x *ModelInfo) GetErrorMessage() string {
	if x != nil && x.ErrorMessage != nil {
		return *x.ErrorMessage
	}
	return ""
}

func (x *ModelInfo) GetAvgLatencyMs() float64 {
	if x != nil && x.AvgLatencyMs != nil {
		return *x.AvgLatencyMs
	}
	return 0
}

func (x *ModelInfo) GetRequestsLastHour() int64 {
	if x != nil && x.RequestsLastHour != nil {
		return *x.RequestsLastHour
	}
	return 0
}

func (x *ModelInfo) GetEndpoint() string {
	if x != nil && x.Endpoint != nil {
		return *x.Endpoint
	}
	return ""
}

func (x *ModelInfo) GetInputCostPer_1K() float64 {
	if x != nil && x.InputCostPer_1K != nil {
		return *x.InputCostPer_1K
	}
	return 0
}

func (x *ModelInfo) GetOutputCostPer_1K() float64 {
	if x != nil && x.OutputCostPer_1K != nil {
		return *x.OutputCostPer_1K
	}
	return 0
}

func (x *ModelInfo) GetPriority() int32 {
	if x != nil {
		return x.Priority
	}
	return 0
}

func (x *ModelInfo) GetIsEnabled() bool {
	if x != nil {
		return x.IsEnabled
	}
	return false
}

// GetModelStatusRequest queries the status of AI models.
type GetModelStatusRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Filter by model name (optional).
	// If not specified, returns all models.
	ModelName *string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3,oneof" json:"model_name,omitempty"`
	// Filter by model type (optional).
	ModelType *ModelType `protobuf:"varint,2,opt,name=model_type,json=modelType,proto3,enum=penfold.ai.v1.ModelType,oneof" json:"model_type,omitempty"`
	// Whether to include detailed metrics.
	// Default: false
	IncludeMetrics *bool `protobuf:"varint,3,opt,name=include_metrics,json=includeMetrics,proto3,oneof" json:"include_metrics,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *GetModelStatusRequest) Reset() {
	*x = GetModelStatusRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[27]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelStatusRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelStatusRequest) ProtoMessage() {}

func (x *GetModelStatusRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[27]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelStatusRequest.ProtoReflect.Descriptor instead.
func (*GetModelStatusRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{27}
}

func (x *GetModelStatusRequest) GetModelName() string {
	if x != nil && x.ModelName != nil {
		return *x.ModelName
	}
	return ""
}

func (x *GetModelStatusRequest) GetModelType() ModelType {
	if x != nil && x.ModelType != nil {
		return *x.ModelType
	}
	return ModelType_MODEL_TYPE_UNSPECIFIED
}

func (x *GetModelStatusRequest) GetIncludeMetrics() bool {
	if x != nil && x.IncludeMetrics != nil {
		return *x.IncludeMetrics
	}
	return false
}

// GetModelStatusResponse contains information about available models.
type GetModelStatusResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of available models matching the request filters.
	Models []*ModelInfo `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	// Default embedding model name.
	DefaultEmbeddingModel string `protobuf:"bytes,2,opt,name=default_embedding_model,json=defaultEmbeddingModel,proto3" json:"default_embedding_model,omitempty"`
	// Default LLM model name.
	DefaultLlmModel string `protobuf:"bytes,3,opt,name=default_llm_model,json=defaultLlmModel,proto3" json:"default_llm_model,omitempty"`
	// Overall health status of the AI coordinator.
	Healthy bool `protobuf:"varint,4,opt,name=healthy,proto3" json:"healthy,omitempty"`
	// Human-readable status message.
	Message       string `protobuf:"bytes,5,opt,name=message,proto3" json:"message,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetModelStatusResponse) Reset() {
	*x = GetModelStatusResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[28]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelStatusResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelStatusResponse) ProtoMessage() {}

func (x *GetModelStatusResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[28]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelStatusResponse.ProtoReflect.Descriptor instead.
func (*GetModelStatusResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{28}
}

func (x *GetModelStatusResponse) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *GetModelStatusResponse) GetDefaultEmbeddingModel() string {
	if x != nil {
		return x.DefaultEmbeddingModel
	}
	return ""
}

func (x *GetModelStatusResponse) GetDefaultLlmModel() string {
	if x != nil {
		return x.DefaultLlmModel
	}
	return ""
}

func (x *GetModelStatusResponse) GetHealthy() bool {
	if x != nil {
		return x.Healthy
	}
	return false
}

func (x *GetModelStatusResponse) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

// ListModelsRequest filters the list of registered models.
type ListModelsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Filter by provider (e.g., "ollama", "openai", "anthropic").
	// If empty, returns models from all providers.
	Provider *string `protobuf:"bytes,1,opt,name=provider,proto3,oneof" json:"provider,omitempty"`
	// Filter by capability (e.g., "embedding", "chat", "summarization").
	// If empty, returns models with any capabilities.
	Capability *string `protobuf:"bytes,2,opt,name=capability,proto3,oneof" json:"capability,omitempty"`
	// Filter by local/cloud deployment.
	// If not specified, returns both local and cloud models.
	IsLocal *bool `protobuf:"varint,3,opt,name=is_local,json=isLocal,proto3,oneof" json:"is_local,omitempty"`
	// Filter by enabled status.
	// If not specified, returns both enabled and disabled models.
	IsEnabled *bool `protobuf:"varint,4,opt,name=is_enabled,json=isEnabled,proto3,oneof" json:"is_enabled,omitempty"`
	// Filter by model type.
	ModelType *ModelType `protobuf:"varint,5,opt,name=model_type,json=modelType,proto3,enum=penfold.ai.v1.ModelType,oneof" json:"model_type,omitempty"`
	// Maximum number of results to return.
	// Default: 100, Maximum: 1000
	PageSize *int32 `protobuf:"varint,6,opt,name=page_size,json=pageSize,proto3,oneof" json:"page_size,omitempty"`
	// Pagination token from a previous response.
	PageToken     *string `protobuf:"bytes,7,opt,name=page_token,json=pageToken,proto3,oneof" json:"page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsRequest) Reset() {
	*x = ListModelsRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[29]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsRequest) ProtoMessage() {}

func (x *ListModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[29]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsRequest.ProtoReflect.Descriptor instead.
func (*ListModelsRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{29}
}

func (x *ListModelsRequest) GetProvider() string {
	if x != nil && x.Provider != nil {
		return *x.Provider
	}
	return ""
}

func (x *ListModelsRequest) GetCapability() string {
	if x != nil && x.Capability != nil {
		return *x.Capability
	}
	return ""
}

func (x *ListModelsRequest) GetIsLocal() bool {
	if x != nil && x.IsLocal != nil {
		return *x.IsLocal
	}
	return false
}

func (x *ListModelsRequest) GetIsEnabled() bool {
	if x != nil && x.IsEnabled != nil {
		return *x.IsEnabled
	}
	return false
}

func (x *ListModelsRequest) GetModelType() ModelType {
	if x != nil && x.ModelType != nil {
		return *x.ModelType
	}
	return ModelType_MODEL_TYPE_UNSPECIFIED
}

func (x *ListModelsRequest) GetPageSize() int32 {
	if x != nil && x.PageSize != nil {
		return *x.PageSize
	}
	return 0
}

func (x *ListModelsRequest) GetPageToken() string {
	if x != nil && x.PageToken != nil {
		return *x.PageToken
	}
	return ""
}

// ListModelsResponse contains the list of matching models.
type ListModelsResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of models matching the request filters.
	Models []*ModelInfo `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	// Token for retrieving the next page of results.
	// Empty if there are no more results.
	NextPageToken string `protobuf:"bytes,2,opt,name=next_page_token,json=nextPageToken,proto3" json:"next_page_token,omitempty"`
	// Total number of models matching the filter (ignoring pagination).
	TotalCount    int32 `protobuf:"varint,3,opt,name=total_count,json=totalCount,proto3" json:"total_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsResponse) Reset() {
	*x = ListModelsResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[30]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsResponse) ProtoMessage() {}

func (x *ListModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[30]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsResponse.ProtoReflect.Descriptor instead.
func (*ListModelsResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{30}
}

func (x *ListModelsResponse) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *ListModelsResponse) GetNextPageToken() string {
	if x != nil {
		return x.NextPageToken
	}
	return ""
}

func (x *ListModelsResponse) GetTotalCount() int32 {
	if x != nil {
		return x.TotalCount
	}
	return 0
}

// RegisterModelRequest contains the configuration for a new model.
type RegisterModelRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Human-readable name for the model.
	// Required. Must be unique within the tenant.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Provider name (e.g., "ollama", "openai", "google", "anthropic").
	// Required.
	Provider string `protobuf:"bytes,2,opt,name=provider,proto3" json:"provider,omitempty"`
	// The provider's model identifier.
	// Required. Examples: "gpt-4-turbo", "nomic-embed-text"
	ModelName string `protobuf:"bytes,3,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// The type of model.
	// Required.
	Type ModelType `protobuf:"varint,4,opt,name=type,proto3,enum=penfold.ai.v1.ModelType" json:"type,omitempty"`
	// API endpoint URL for the model.
	// Optional. Uses provider default if not specified.
	Endpoint *string `protobuf:"bytes,5,opt,name=endpoint,proto3,oneof" json:"endpoint,omitempty"`
	// List of capabilities this model supports.
	// Required. At least one capability must be specified.
	Capabilities []string `protobuf:"bytes,6,rep,name=capabilities,proto3" json:"capabilities,omitempty"`
	// Maximum context length in tokens.
	// Optional. Uses provider default if not specified.
	MaxContextLength *int32 `protobuf:"varint,7,opt,name=max_context_length,json=maxContextLength,proto3,oneof" json:"max_context_length,omitempty"`
	// Embedding dimensions (required for embedding models).
	EmbeddingDimensions *int32 `protobuf:"varint,8,opt,name=embedding_dimensions,json=embeddingDimensions,proto3,oneof" json:"embedding_dimensions,omitempty"`
	// Whether this is a local model.
	// Default: false (cloud API)
	IsLocal bool `protobuf:"varint,9,opt,name=is_local,json=isLocal,proto3" json:"is_local,omitempty"`
	// Model version or variant.
	Version *string `protobuf:"bytes,10,opt,name=version,proto3,oneof" json:"version,omitempty"`
	// Cost per 1,000 input tokens in USD.
	InputCostPer_1K *float64 `protobuf:"fixed64,11,opt,name=input_cost_per_1k,json=inputCostPer1k,proto3,oneof" json:"input_cost_per_1k,omitempty"`
	// Cost per 1,000 output tokens in USD.
	OutputCostPer_1K *float64 `protobuf:"fixed64,12,opt,name=output_cost_per_1k,json=outputCostPer1k,proto3,oneof" json:"output_cost_per_1k,omitempty"`
	// Priority for model selection (higher = preferred).
	// Default: 0
	Priority int32 `protobuf:"varint,13,opt,name=priority,proto3" json:"priority,omitempty"`
	// Whether this model is enabled for use.
	// Default: true
	IsEnabled     bool `protobuf:"varint,14,opt,name=is_enabled,json=isEnabled,proto3" json:"is_enabled,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RegisterModelRequest) Reset() {
	*x = RegisterModelRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[31]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RegisterModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RegisterModelRequest) ProtoMessage() {}

func (x *RegisterModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[31]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RegisterModelRequest.ProtoReflect.Descriptor instead.
func (*RegisterModelRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{31}
}

func (x *RegisterModelRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *RegisterModelRequest) GetProvider() string {
	if x != nil {
		return x.Provider
	}
	return ""
}

func (x *RegisterModelRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *RegisterModelRequest) GetType() ModelType {
	if x != nil {
		return x.Type
	}
	return ModelType_MODEL_TYPE_UNSPECIFIED
}

func (x *RegisterModelRequest) GetEndpoint() string {
	if x != nil && x.Endpoint != nil {
		return *x.Endpoint
	}
	return ""
}

func (x *RegisterModelRequest) GetCapabilities() []string {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

func (x *RegisterModelRequest) GetMaxContextLength() int32 {
	if x != nil && x.MaxContextLength != nil {
		return *x.MaxContextLength
	}
	return 0
}

func (x *RegisterModelRequest) GetEmbeddingDimensions() int32 {
	if x != nil && x.EmbeddingDimensions != nil {
		return *x.EmbeddingDimensions
	}
	return 0
}

func (x *RegisterModelRequest) GetIsLocal() bool {
	if x != nil {
		return x.IsLocal
	}
	return false
}

func (x *RegisterModelRequest) GetVersion() string {
	if x != nil && x.Version != nil {
		return *x.Version
	}
	return ""
}

func (x *RegisterModelRequest) GetInputCostPer_1K() float64 {
	if x != nil && x.InputCostPer_1K != nil {
		return *x.InputCostPer_1K
	}
	return 0
}

func (x *RegisterModelRequest) GetOutputCostPer_1K() float64 {
	if x != nil && x.OutputCostPer_1K != nil {
		return *x.OutputCostPer_1K
	}
	return 0
}

func (x *RegisterModelRequest) GetPriority() int32 {
	if x != nil {
		return x.Priority
	}
	return 0
}

func (x *RegisterModelRequest) GetIsEnabled() bool {
	if x != nil {
		return x.IsEnabled
	}
	return false
}

// RegisterModelResponse contains the registered model.
type RegisterModelResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The registered model with its assigned ID.
	Model         *ModelInfo `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RegisterModelResponse) Reset() {
	*x = RegisterModelResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[32]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RegisterModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RegisterModelResponse) ProtoMessage() {}

func (x *RegisterModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[32]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RegisterModelResponse.ProtoReflect.Descriptor instead.
func (*RegisterModelResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{32}
}

func (x *RegisterModelResponse) GetModel() *ModelInfo {
	if x != nil {
		return x.Model
	}
	return nil
}

// UpdateModelRequest specifies which model to update and the new values.
type UpdateModelRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The ID of the model to update.
	// Required.
	ModelId string `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	// New human-readable name for the model.
	// If not set, the existing name is preserved.
	Name *string `protobuf:"bytes,2,opt,name=name,proto3,oneof" json:"name,omitempty"`
	// New API endpoint URL.
	// If not set, the existing endpoint is preserved.
	Endpoint *string `protobuf:"bytes,3,opt,name=endpoint,proto3,oneof" json:"endpoint,omitempty"`
	// New list of capabilities.
	// If not set, the existing capabilities are preserved.
	// To clear capabilities, set to an empty list.
	Capabilities []string `protobuf:"bytes,4,rep,name=capabilities,proto3" json:"capabilities,omitempty"`
	// New maximum context length.
	// If not set, the existing value is preserved.
	MaxContextLength *int32 `protobuf:"varint,5,opt,name=max_context_length,json=maxContextLength,proto3,oneof" json:"max_context_length,omitempty"`
	// New embedding dimensions.
	// If not set, the existing value is preserved.
	EmbeddingDimensions *int32 `protobuf:"varint,6,opt,name=embedding_dimensions,json=embeddingDimensions,proto3,oneof" json:"embedding_dimensions,omitempty"`
	// New cost per 1,000 input tokens.
	// If not set, the existing value is preserved.
	InputCostPer_1K *float64 `protobuf:"fixed64,7,opt,name=input_cost_per_1k,json=inputCostPer1k,proto3,oneof" json:"input_cost_per_1k,omitempty"`
	// New cost per 1,000 output tokens.
	// If not set, the existing value is preserved.
	OutputCostPer_1K *float64 `protobuf:"fixed64,8,opt,name=output_cost_per_1k,json=outputCostPer1k,proto3,oneof" json:"output_cost_per_1k,omitempty"`
	// New priority for model selection.
	// If not set, the existing value is preserved.
	Priority *int32 `protobuf:"varint,9,opt,name=priority,proto3,oneof" json:"priority,omitempty"`
	// New enabled status.
	// If not set, the existing value is preserved.
	IsEnabled     *bool `protobuf:"varint,10,opt,name=is_enabled,json=isEnabled,proto3,oneof" json:"is_enabled,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UpdateModelRequest) Reset() {
	*x = UpdateModelRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[33]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UpdateModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UpdateModelRequest) ProtoMessage() {}

func (x *UpdateModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[33]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UpdateModelRequest.ProtoReflect.Descriptor instead.
func (*UpdateModelRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{33}
}

func (x *UpdateModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *UpdateModelRequest) GetName() string {
	if x != nil && x.Name != nil {
		return *x.Name
	}
	return ""
}

func (x *UpdateModelRequest) GetEndpoint() string {
	if x != nil && x.Endpoint != nil {
		return *x.Endpoint
	}
	return ""
}

func (x *UpdateModelRequest) GetCapabilities() []string {
	if x != nil {
		return x.Capabilities
	}
	return nil
}

func (x *UpdateModelRequest) GetMaxContextLength() int32 {
	if x != nil && x.MaxContextLength != nil {
		return *x.MaxContextLength
	}
	return 0
}

func (x *UpdateModelRequest) GetEmbeddingDimensions() int32 {
	if x != nil && x.EmbeddingDimensions != nil {
		return *x.EmbeddingDimensions
	}
	return 0
}

func (x *UpdateModelRequest) GetInputCostPer_1K() float64 {
	if x != nil && x.InputCostPer_1K != nil {
		return *x.InputCostPer_1K
	}
	return 0
}

func (x *UpdateModelRequest) GetOutputCostPer_1K() float64 {
	if x != nil && x.OutputCostPer_1K != nil {
		return *x.OutputCostPer_1K
	}
	return 0
}

func (x *UpdateModelRequest) GetPriority() int32 {
	if x != nil && x.Priority != nil {
		return *x.Priority
	}
	return 0
}

func (x *UpdateModelRequest) GetIsEnabled() bool {
	if x != nil && x.IsEnabled != nil {
		return *x.IsEnabled
	}
	return false
}

// UpdateModelResponse contains the updated model.
type UpdateModelResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The updated model configuration.
	Model         *ModelInfo `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UpdateModelResponse) Reset() {
	*x = UpdateModelResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[34]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UpdateModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UpdateModelResponse) ProtoMessage() {}

func (x *UpdateModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[34]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UpdateModelResponse.ProtoReflect.Descriptor instead.
func (*UpdateModelResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{34}
}

func (x *UpdateModelResponse) GetModel() *ModelInfo {
	if x != nil {
		return x.Model
	}
	return nil
}

// DeleteModelRequest specifies which model to remove.
type DeleteModelRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The ID of the model to delete.
	// Required.
	ModelId       string `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DeleteModelRequest) Reset() {
	*x = DeleteModelRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[35]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeleteModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeleteModelRequest) ProtoMessage() {}

func (x *DeleteModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[35]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeleteModelRequest.ProtoReflect.Descriptor instead.
func (*DeleteModelRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{35}
}

func (x *DeleteModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

// DeleteModelResponse confirms the deletion.
type DeleteModelResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// True if the model was deleted, false if it didn't exist.
	Deleted bool `protobuf:"varint,1,opt,name=deleted,proto3" json:"deleted,omitempty"`
	// Human-readable message about the deletion.
	Message       string `protobuf:"bytes,2,opt,name=message,proto3" json:"message,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DeleteModelResponse) Reset() {
	*x = DeleteModelResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[36]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeleteModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeleteModelResponse) ProtoMessage() {}

func (x *DeleteModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[36]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeleteModelResponse.ProtoReflect.Descriptor instead.
func (*DeleteModelResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{36}
}

func (x *DeleteModelResponse) GetDeleted() bool {
	if x != nil {
		return x.Deleted
	}
	return false
}

func (x *DeleteModelResponse) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

// RoutingRule defines how requests are routed to models.
type RoutingRule struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Unique identifier for this rule.
	Id string `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// Human-readable name for the rule.
	// Examples: "embedding-routing", "summarization-high-quality"
	Name string `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`
	// The task type this rule applies to.
	// Examples: "embedding", "summarization", "extraction", "classification"
	TaskType string `protobuf:"bytes,3,opt,name=task_type,json=taskType,proto3" json:"task_type,omitempty"`
	// Ordered list of preferred model IDs.
	// The router tries these models in order until one succeeds.
	PreferredModelIds []string `protobuf:"bytes,4,rep,name=preferred_model_ids,json=preferredModelIds,proto3" json:"preferred_model_ids,omitempty"`
	// Ordered list of fallback model IDs.
	// Used if all preferred models fail or are unavailable.
	FallbackModelIds []string `protobuf:"bytes,5,rep,name=fallback_model_ids,json=fallbackModelIds,proto3" json:"fallback_model_ids,omitempty"`
	// Optimization mode for model selection within this rule.
	OptimizationMode OptimizationMode `protobuf:"varint,6,opt,name=optimization_mode,json=optimizationMode,proto3,enum=penfold.ai.v1.OptimizationMode" json:"optimization_mode,omitempty"`
	// Whether this rule is enabled.
	// Disabled rules are ignored during routing.
	IsEnabled bool `protobuf:"varint,7,opt,name=is_enabled,json=isEnabled,proto3" json:"is_enabled,omitempty"`
	// Optional description of the rule's purpose.
	Description *string `protobuf:"bytes,8,opt,name=description,proto3,oneof" json:"description,omitempty"`
	// When this rule was created (RFC 3339 timestamp).
	CreatedAt string `protobuf:"bytes,9,opt,name=created_at,json=createdAt,proto3" json:"created_at,omitempty"`
	// When this rule was last updated (RFC 3339 timestamp).
	UpdatedAt     string `protobuf:"bytes,10,opt,name=updated_at,json=updatedAt,proto3" json:"updated_at,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RoutingRule) Reset() {
	*x = RoutingRule{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[37]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RoutingRule) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RoutingRule) ProtoMessage() {}

func (x *RoutingRule) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[37]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RoutingRule.ProtoReflect.Descriptor instead.
func (*RoutingRule) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{37}
}

func (x *RoutingRule) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *RoutingRule) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *RoutingRule) GetTaskType() string {
	if x != nil {
		return x.TaskType
	}
	return ""
}

func (x *RoutingRule) GetPreferredModelIds() []string {
	if x != nil {
		return x.PreferredModelIds
	}
	return nil
}

func (x *RoutingRule) GetFallbackModelIds() []string {
	if x != nil {
		return x.FallbackModelIds
	}
	return nil
}

func (x *RoutingRule) GetOptimizationMode() OptimizationMode {
	if x != nil {
		return x.OptimizationMode
	}
	return OptimizationMode_OPTIMIZATION_MODE_UNSPECIFIED
}

func (x *RoutingRule) GetIsEnabled() bool {
	if x != nil {
		return x.IsEnabled
	}
	return false
}

func (x *RoutingRule) GetDescription() string {
	if x != nil && x.Description != nil {
		return *x.Description
	}
	return ""
}

func (x *RoutingRule) GetCreatedAt() string {
	if x != nil {
		return x.CreatedAt
	}
	return ""
}

func (x *RoutingRule) GetUpdatedAt() string {
	if x != nil {
		return x.UpdatedAt
	}
	return ""
}

// GetRoutingRulesRequest filters the list of routing rules.
type GetRoutingRulesRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Filter by task type.
	// If empty, returns rules for all task types.
	TaskType *string `protobuf:"bytes,1,opt,name=task_type,json=taskType,proto3,oneof" json:"task_type,omitempty"`
	// Filter by enabled status.
	// If not specified, returns both enabled and disabled rules.
	IsEnabled     *bool `protobuf:"varint,2,opt,name=is_enabled,json=isEnabled,proto3,oneof" json:"is_enabled,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetRoutingRulesRequest) Reset() {
	*x = GetRoutingRulesRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[38]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetRoutingRulesRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetRoutingRulesRequest) ProtoMessage() {}

func (x *GetRoutingRulesRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[38]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetRoutingRulesRequest.ProtoReflect.Descriptor instead.
func (*GetRoutingRulesRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{38}
}

func (x *GetRoutingRulesRequest) GetTaskType() string {
	if x != nil && x.TaskType != nil {
		return *x.TaskType
	}
	return ""
}

func (x *GetRoutingRulesRequest) GetIsEnabled() bool {
	if x != nil && x.IsEnabled != nil {
		return *x.IsEnabled
	}
	return false
}

// GetRoutingRulesResponse contains the matching routing rules.
type GetRoutingRulesResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of routing rules matching the request filters.
	Rules         []*RoutingRule `protobuf:"bytes,1,rep,name=rules,proto3" json:"rules,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetRoutingRulesResponse) Reset() {
	*x = GetRoutingRulesResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[39]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetRoutingRulesResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetRoutingRulesResponse) ProtoMessage() {}

func (x *GetRoutingRulesResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[39]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetRoutingRulesResponse.ProtoReflect.Descriptor instead.
func (*GetRoutingRulesResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{39}
}

func (x *GetRoutingRulesResponse) GetRules() []*RoutingRule {
	if x != nil {
		return x.Rules
	}
	return nil
}

// UpdateRoutingRuleRequest creates or updates a routing rule.
type UpdateRoutingRuleRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Human-readable name for the rule.
	// Required. If a rule with this name exists, it is updated.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// The task type this rule applies to.
	// Required.
	TaskType string `protobuf:"bytes,2,opt,name=task_type,json=taskType,proto3" json:"task_type,omitempty"`
	// Ordered list of preferred model IDs.
	// Required. At least one model ID must be specified.
	PreferredModelIds []string `protobuf:"bytes,3,rep,name=preferred_model_ids,json=preferredModelIds,proto3" json:"preferred_model_ids,omitempty"`
	// Ordered list of fallback model IDs.
	// Optional. Used when preferred models are unavailable.
	FallbackModelIds []string `protobuf:"bytes,4,rep,name=fallback_model_ids,json=fallbackModelIds,proto3" json:"fallback_model_ids,omitempty"`
	// Optimization mode for model selection.
	// Default: OPTIMIZATION_MODE_BALANCED
	OptimizationMode OptimizationMode `protobuf:"varint,5,opt,name=optimization_mode,json=optimizationMode,proto3,enum=penfold.ai.v1.OptimizationMode" json:"optimization_mode,omitempty"`
	// Whether this rule is enabled.
	// Default: true
	IsEnabled bool `protobuf:"varint,6,opt,name=is_enabled,json=isEnabled,proto3" json:"is_enabled,omitempty"`
	// Optional description of the rule's purpose.
	Description   *string `protobuf:"bytes,7,opt,name=description,proto3,oneof" json:"description,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UpdateRoutingRuleRequest) Reset() {
	*x = UpdateRoutingRuleRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[40]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UpdateRoutingRuleRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UpdateRoutingRuleRequest) ProtoMessage() {}

func (x *UpdateRoutingRuleRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[40]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UpdateRoutingRuleRequest.ProtoReflect.Descriptor instead.
func (*UpdateRoutingRuleRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{40}
}

func (x *UpdateRoutingRuleRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *UpdateRoutingRuleRequest) GetTaskType() string {
	if x != nil {
		return x.TaskType
	}
	return ""
}

func (x *UpdateRoutingRuleRequest) GetPreferredModelIds() []string {
	if x != nil {
		return x.PreferredModelIds
	}
	return nil
}

func (x *UpdateRoutingRuleRequest) GetFallbackModelIds() []string {
	if x != nil {
		return x.FallbackModelIds
	}
	return nil
}

func (x *UpdateRoutingRuleRequest) GetOptimizationMode() OptimizationMode {
	if x != nil {
		return x.OptimizationMode
	}
	return OptimizationMode_OPTIMIZATION_MODE_UNSPECIFIED
}

func (x *UpdateRoutingRuleRequest) GetIsEnabled() bool {
	if x != nil {
		return x.IsEnabled
	}
	return false
}

func (x *UpdateRoutingRuleRequest) GetDescription() string {
	if x != nil && x.Description != nil {
		return *x.Description
	}
	return ""
}

// UpdateRoutingRuleResponse contains the created or updated rule.
type UpdateRoutingRuleResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The routing rule that was created or updated.
	Rule *RoutingRule `protobuf:"bytes,1,opt,name=rule,proto3" json:"rule,omitempty"`
	// True if a new rule was created, false if an existing rule was updated.
	Created       bool `protobuf:"varint,2,opt,name=created,proto3" json:"created,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UpdateRoutingRuleResponse) Reset() {
	*x = UpdateRoutingRuleResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[41]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UpdateRoutingRuleResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UpdateRoutingRuleResponse) ProtoMessage() {}

func (x *UpdateRoutingRuleResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[41]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UpdateRoutingRuleResponse.ProtoReflect.Descriptor instead.
func (*UpdateRoutingRuleResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{41}
}

func (x *UpdateRoutingRuleResponse) GetRule() *RoutingRule {
	if x != nil {
		return x.Rule
	}
	return nil
}

func (x *UpdateRoutingRuleResponse) GetCreated() bool {
	if x != nil {
		return x.Created
	}
	return false
}

// QueryRequest contains the question for RAG-style Q&A over the knowledge base.
type QueryRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The question to answer using the knowledge base.
	Question string `protobuf:"bytes,1,opt,name=question,proto3" json:"question,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,2,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Maximum number of source documents to consider for context (default: 5).
	ContextLimit *int32 `protobuf:"varint,3,opt,name=context_limit,json=contextLimit,proto3,oneof" json:"context_limit,omitempty"`
	// The LLM model to use for answering.
	// If not specified, the default model will be used.
	Model *string `protobuf:"bytes,4,opt,name=model,proto3,oneof" json:"model,omitempty"`
	// Maximum tokens in the response (default: 1000).
	MaxTokens *int32 `protobuf:"varint,5,opt,name=max_tokens,json=maxTokens,proto3,oneof" json:"max_tokens,omitempty"`
	// Response creativity (0.0 to 1.0, default: 0.7).
	Temperature   *float32 `protobuf:"fixed32,6,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *QueryRequest) Reset() {
	*x = QueryRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[42]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *QueryRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*QueryRequest) ProtoMessage() {}

func (x *QueryRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[42]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use QueryRequest.ProtoReflect.Descriptor instead.
func (*QueryRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{42}
}

func (x *QueryRequest) GetQuestion() string {
	if x != nil {
		return x.Question
	}
	return ""
}

func (x *QueryRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *QueryRequest) GetContextLimit() int32 {
	if x != nil && x.ContextLimit != nil {
		return *x.ContextLimit
	}
	return 0
}

func (x *QueryRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

func (x *QueryRequest) GetMaxTokens() int32 {
	if x != nil && x.MaxTokens != nil {
		return *x.MaxTokens
	}
	return 0
}

func (x *QueryRequest) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

// QuerySource represents a source document used to answer a query.
type QuerySource struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Unique identifier of the source document.
	SourceId string `protobuf:"bytes,1,opt,name=source_id,json=sourceId,proto3" json:"source_id,omitempty"`
	// Title or subject of the source document.
	Title string `protobuf:"bytes,2,opt,name=title,proto3" json:"title,omitempty"`
	// Type of content: "email", "document", "meeting", etc.
	ContentType string `protobuf:"bytes,3,opt,name=content_type,json=contentType,proto3" json:"content_type,omitempty"`
	// Relevance score (0.0 to 1.0).
	Relevance float32 `protobuf:"fixed32,4,opt,name=relevance,proto3" json:"relevance,omitempty"`
	// Snippet showing relevant content.
	Snippet       *string `protobuf:"bytes,5,opt,name=snippet,proto3,oneof" json:"snippet,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *QuerySource) Reset() {
	*x = QuerySource{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[43]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *QuerySource) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*QuerySource) ProtoMessage() {}

func (x *QuerySource) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[43]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use QuerySource.ProtoReflect.Descriptor instead.
func (*QuerySource) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{43}
}

func (x *QuerySource) GetSourceId() string {
	if x != nil {
		return x.SourceId
	}
	return ""
}

func (x *QuerySource) GetTitle() string {
	if x != nil {
		return x.Title
	}
	return ""
}

func (x *QuerySource) GetContentType() string {
	if x != nil {
		return x.ContentType
	}
	return ""
}

func (x *QuerySource) GetRelevance() float32 {
	if x != nil {
		return x.Relevance
	}
	return 0
}

func (x *QuerySource) GetSnippet() string {
	if x != nil && x.Snippet != nil {
		return *x.Snippet
	}
	return ""
}

// QueryResponse contains the answer and its sources.
type QueryResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Unique identifier for this query response.
	ResponseId string `protobuf:"bytes,1,opt,name=response_id,json=responseId,proto3" json:"response_id,omitempty"`
	// The generated answer to the question.
	Answer string `protobuf:"bytes,2,opt,name=answer,proto3" json:"answer,omitempty"`
	// Source documents that informed the answer.
	Sources []*QuerySource `protobuf:"bytes,3,rep,name=sources,proto3" json:"sources,omitempty"`
	// The model that was used to generate the answer.
	ModelUsed string `protobuf:"bytes,4,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Token count of the input (question + context).
	InputTokens *int32 `protobuf:"varint,5,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	// Token count of the generated answer.
	OutputTokens *int32 `protobuf:"varint,6,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	// Query execution time in milliseconds.
	LatencyMs     *float64 `protobuf:"fixed64,7,opt,name=latency_ms,json=latencyMs,proto3,oneof" json:"latency_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *QueryResponse) Reset() {
	*x = QueryResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[44]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *QueryResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*QueryResponse) ProtoMessage() {}

func (x *QueryResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[44]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use QueryResponse.ProtoReflect.Descriptor instead.
func (*QueryResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{44}
}

func (x *QueryResponse) GetResponseId() string {
	if x != nil {
		return x.ResponseId
	}
	return ""
}

func (x *QueryResponse) GetAnswer() string {
	if x != nil {
		return x.Answer
	}
	return ""
}

func (x *QueryResponse) GetSources() []*QuerySource {
	if x != nil {
		return x.Sources
	}
	return nil
}

func (x *QueryResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *QueryResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *QueryResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

func (x *QueryResponse) GetLatencyMs() float64 {
	if x != nil && x.LatencyMs != nil {
		return *x.LatencyMs
	}
	return 0
}

// SummarizeByIDRequest requests a summary of content by its ID.
type SummarizeByIDRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content ID to summarize.
	ContentId string `protobuf:"bytes,1,opt,name=content_id,json=contentId,proto3" json:"content_id,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,2,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Summary length: "brief", "standard", or "detailed" (default: "standard").
	Length *string `protobuf:"bytes,3,opt,name=length,proto3,oneof" json:"length,omitempty"`
	// The LLM model to use for summarization.
	// If not specified, the default model will be used.
	Model         *string `protobuf:"bytes,4,opt,name=model,proto3,oneof" json:"model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SummarizeByIDRequest) Reset() {
	*x = SummarizeByIDRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[45]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummarizeByIDRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummarizeByIDRequest) ProtoMessage() {}

func (x *SummarizeByIDRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[45]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummarizeByIDRequest.ProtoReflect.Descriptor instead.
func (*SummarizeByIDRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{45}
}

func (x *SummarizeByIDRequest) GetContentId() string {
	if x != nil {
		return x.ContentId
	}
	return ""
}

func (x *SummarizeByIDRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *SummarizeByIDRequest) GetLength() string {
	if x != nil && x.Length != nil {
		return *x.Length
	}
	return ""
}

func (x *SummarizeByIDRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

// SummarizeByIDResponse contains the generated summary.
type SummarizeByIDResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Unique identifier for this summary response.
	ResponseId string `protobuf:"bytes,1,opt,name=response_id,json=responseId,proto3" json:"response_id,omitempty"`
	// The content ID that was summarized.
	ContentId string `protobuf:"bytes,2,opt,name=content_id,json=contentId,proto3" json:"content_id,omitempty"`
	// The generated summary.
	Summary string `protobuf:"bytes,3,opt,name=summary,proto3" json:"summary,omitempty"`
	// Key points extracted from the content.
	KeyPoints []string `protobuf:"bytes,4,rep,name=key_points,json=keyPoints,proto3" json:"key_points,omitempty"`
	// The model that was used to generate the summary.
	ModelUsed string `protobuf:"bytes,5,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Content type of the summarized content.
	ContentType string `protobuf:"bytes,6,opt,name=content_type,json=contentType,proto3" json:"content_type,omitempty"`
	// Token count of the input content.
	InputTokens *int32 `protobuf:"varint,7,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	// Token count of the generated summary.
	OutputTokens *int32 `protobuf:"varint,8,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	// Execution time in milliseconds.
	LatencyMs     *float64 `protobuf:"fixed64,9,opt,name=latency_ms,json=latencyMs,proto3,oneof" json:"latency_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SummarizeByIDResponse) Reset() {
	*x = SummarizeByIDResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[46]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummarizeByIDResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummarizeByIDResponse) ProtoMessage() {}

func (x *SummarizeByIDResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[46]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummarizeByIDResponse.ProtoReflect.Descriptor instead.
func (*SummarizeByIDResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{46}
}

func (x *SummarizeByIDResponse) GetResponseId() string {
	if x != nil {
		return x.ResponseId
	}
	return ""
}

func (x *SummarizeByIDResponse) GetContentId() string {
	if x != nil {
		return x.ContentId
	}
	return ""
}

func (x *SummarizeByIDResponse) GetSummary() string {
	if x != nil {
		return x.Summary
	}
	return ""
}

func (x *SummarizeByIDResponse) GetKeyPoints() []string {
	if x != nil {
		return x.KeyPoints
	}
	return nil
}

func (x *SummarizeByIDResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *SummarizeByIDResponse) GetContentType() string {
	if x != nil {
		return x.ContentType
	}
	return ""
}

func (x *SummarizeByIDResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *SummarizeByIDResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

func (x *SummarizeByIDResponse) GetLatencyMs() float64 {
	if x != nil && x.LatencyMs != nil {
		return *x.LatencyMs
	}
	return 0
}

// AnalyzeByIDRequest requests deep analysis of content by its ID.
type AnalyzeByIDRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The content ID to analyze.
	ContentId string `protobuf:"bytes,1,opt,name=content_id,json=contentId,proto3" json:"content_id,omitempty"`
	// Optional tenant identifier for multi-tenancy support.
	TenantId *string `protobuf:"bytes,2,opt,name=tenant_id,json=tenantId,proto3,oneof" json:"tenant_id,omitempty"`
	// Type of analysis to perform (default: FULL).
	AnalysisType AnalysisType `protobuf:"varint,3,opt,name=analysis_type,json=analysisType,proto3,enum=penfold.ai.v1.AnalysisType" json:"analysis_type,omitempty"`
	// The LLM model to use for analysis.
	// If not specified, the default model will be used.
	Model         *string `protobuf:"bytes,4,opt,name=model,proto3,oneof" json:"model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AnalyzeByIDRequest) Reset() {
	*x = AnalyzeByIDRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[47]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalyzeByIDRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalyzeByIDRequest) ProtoMessage() {}

func (x *AnalyzeByIDRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[47]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalyzeByIDRequest.ProtoReflect.Descriptor instead.
func (*AnalyzeByIDRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{47}
}

func (x *AnalyzeByIDRequest) GetContentId() string {
	if x != nil {
		return x.ContentId
	}
	return ""
}

func (x *AnalyzeByIDRequest) GetTenantId() string {
	if x != nil && x.TenantId != nil {
		return *x.TenantId
	}
	return ""
}

func (x *AnalyzeByIDRequest) GetAnalysisType() AnalysisType {
	if x != nil {
		return x.AnalysisType
	}
	return AnalysisType_ANALYSIS_TYPE_UNSPECIFIED
}

func (x *AnalyzeByIDRequest) GetModel() string {
	if x != nil && x.Model != nil {
		return *x.Model
	}
	return ""
}

// SentimentResult contains sentiment analysis results.
type SentimentResult struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Overall sentiment score (-1.0 negative to 1.0 positive).
	Score float32 `protobuf:"fixed32,1,opt,name=score,proto3" json:"score,omitempty"`
	// Sentiment label: "positive", "negative", "neutral", "mixed".
	Label string `protobuf:"bytes,2,opt,name=label,proto3" json:"label,omitempty"`
	// Confidence in the sentiment analysis (0.0 to 1.0).
	Confidence float32 `protobuf:"fixed32,3,opt,name=confidence,proto3" json:"confidence,omitempty"`
	// Key sentiment indicators found in the content.
	Indicators    []string `protobuf:"bytes,4,rep,name=indicators,proto3" json:"indicators,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SentimentResult) Reset() {
	*x = SentimentResult{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[48]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SentimentResult) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SentimentResult) ProtoMessage() {}

func (x *SentimentResult) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[48]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SentimentResult.ProtoReflect.Descriptor instead.
func (*SentimentResult) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{48}
}

func (x *SentimentResult) GetScore() float32 {
	if x != nil {
		return x.Score
	}
	return 0
}

func (x *SentimentResult) GetLabel() string {
	if x != nil {
		return x.Label
	}
	return ""
}

func (x *SentimentResult) GetConfidence() float32 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

func (x *SentimentResult) GetIndicators() []string {
	if x != nil {
		return x.Indicators
	}
	return nil
}

// ExtractedEntity represents an entity found in content.
type ExtractedEntity struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of the entity.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Type of entity: "person", "organization", "location", "date", "project".
	EntityType string `protobuf:"bytes,2,opt,name=entity_type,json=entityType,proto3" json:"entity_type,omitempty"`
	// Number of times this entity appears.
	MentionCount int32 `protobuf:"varint,3,opt,name=mention_count,json=mentionCount,proto3" json:"mention_count,omitempty"`
	// Role or context of the entity (if known).
	Role          *string `protobuf:"bytes,4,opt,name=role,proto3,oneof" json:"role,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ExtractedEntity) Reset() {
	*x = ExtractedEntity{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[49]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExtractedEntity) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExtractedEntity) ProtoMessage() {}

func (x *ExtractedEntity) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[49]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExtractedEntity.ProtoReflect.Descriptor instead.
func (*ExtractedEntity) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{49}
}

func (x *ExtractedEntity) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ExtractedEntity) GetEntityType() string {
	if x != nil {
		return x.EntityType
	}
	return ""
}

func (x *ExtractedEntity) GetMentionCount() int32 {
	if x != nil {
		return x.MentionCount
	}
	return 0
}

func (x *ExtractedEntity) GetRole() string {
	if x != nil && x.Role != nil {
		return *x.Role
	}
	return ""
}

// TopicResult represents a topic identified in content.
type TopicResult struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Topic name or label.
	Topic string `protobuf:"bytes,1,opt,name=topic,proto3" json:"topic,omitempty"`
	// Confidence score (0.0 to 1.0).
	Confidence float32 `protobuf:"fixed32,2,opt,name=confidence,proto3" json:"confidence,omitempty"`
	// Keywords associated with this topic.
	Keywords      []string `protobuf:"bytes,3,rep,name=keywords,proto3" json:"keywords,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TopicResult) Reset() {
	*x = TopicResult{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[50]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TopicResult) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TopicResult) ProtoMessage() {}

func (x *TopicResult) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[50]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TopicResult.ProtoReflect.Descriptor instead.
func (*TopicResult) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{50}
}

func (x *TopicResult) GetTopic() string {
	if x != nil {
		return x.Topic
	}
	return ""
}

func (x *TopicResult) GetConfidence() float32 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

func (x *TopicResult) GetKeywords() []string {
	if x != nil {
		return x.Keywords
	}
	return nil
}

// ActionItem represents an action item extracted from content.
type ActionItem struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Description of the action item.
	Description string `protobuf:"bytes,1,opt,name=description,proto3" json:"description,omitempty"`
	// Priority: "high", "medium", "low".
	Priority *string `protobuf:"bytes,2,opt,name=priority,proto3,oneof" json:"priority,omitempty"`
	// Assigned person (if mentioned).
	Assignee *string `protobuf:"bytes,3,opt,name=assignee,proto3,oneof" json:"assignee,omitempty"`
	// Due date (if mentioned).
	DueDate       *string `protobuf:"bytes,4,opt,name=due_date,json=dueDate,proto3,oneof" json:"due_date,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ActionItem) Reset() {
	*x = ActionItem{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[51]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ActionItem) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ActionItem) ProtoMessage() {}

func (x *ActionItem) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[51]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ActionItem.ProtoReflect.Descriptor instead.
func (*ActionItem) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{51}
}

func (x *ActionItem) GetDescription() string {
	if x != nil {
		return x.Description
	}
	return ""
}

func (x *ActionItem) GetPriority() string {
	if x != nil && x.Priority != nil {
		return *x.Priority
	}
	return ""
}

func (x *ActionItem) GetAssignee() string {
	if x != nil && x.Assignee != nil {
		return *x.Assignee
	}
	return ""
}

func (x *ActionItem) GetDueDate() string {
	if x != nil && x.DueDate != nil {
		return *x.DueDate
	}
	return ""
}

// AnalyzeByIDResponse contains the analysis results.
type AnalyzeByIDResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Unique identifier for this analysis response.
	ResponseId string `protobuf:"bytes,1,opt,name=response_id,json=responseId,proto3" json:"response_id,omitempty"`
	// The content ID that was analyzed.
	ContentId string `protobuf:"bytes,2,opt,name=content_id,json=contentId,proto3" json:"content_id,omitempty"`
	// Type of analysis performed.
	AnalysisType AnalysisType `protobuf:"varint,3,opt,name=analysis_type,json=analysisType,proto3,enum=penfold.ai.v1.AnalysisType" json:"analysis_type,omitempty"`
	// Content type of the analyzed content.
	ContentType string `protobuf:"bytes,4,opt,name=content_type,json=contentType,proto3" json:"content_type,omitempty"`
	// Brief summary of the content (always included).
	Summary string `protobuf:"bytes,5,opt,name=summary,proto3" json:"summary,omitempty"`
	// Sentiment analysis results (if requested or full analysis).
	Sentiment *SentimentResult `protobuf:"bytes,6,opt,name=sentiment,proto3,oneof" json:"sentiment,omitempty"`
	// Extracted entities (if requested or full analysis).
	Entities []*ExtractedEntity `protobuf:"bytes,7,rep,name=entities,proto3" json:"entities,omitempty"`
	// Identified topics (if requested or full analysis).
	Topics []*TopicResult `protobuf:"bytes,8,rep,name=topics,proto3" json:"topics,omitempty"`
	// Extracted action items (if requested or full analysis).
	ActionItems []*ActionItem `protobuf:"bytes,9,rep,name=action_items,json=actionItems,proto3" json:"action_items,omitempty"`
	// Additional insights or recommendations.
	Insights []string `protobuf:"bytes,10,rep,name=insights,proto3" json:"insights,omitempty"`
	// The model that was used for analysis.
	ModelUsed string `protobuf:"bytes,11,opt,name=model_used,json=modelUsed,proto3" json:"model_used,omitempty"`
	// Token count of the input content.
	InputTokens *int32 `protobuf:"varint,12,opt,name=input_tokens,json=inputTokens,proto3,oneof" json:"input_tokens,omitempty"`
	// Token count of the analysis output.
	OutputTokens *int32 `protobuf:"varint,13,opt,name=output_tokens,json=outputTokens,proto3,oneof" json:"output_tokens,omitempty"`
	// Execution time in milliseconds.
	LatencyMs     *float64 `protobuf:"fixed64,14,opt,name=latency_ms,json=latencyMs,proto3,oneof" json:"latency_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AnalyzeByIDResponse) Reset() {
	*x = AnalyzeByIDResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[52]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AnalyzeByIDResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AnalyzeByIDResponse) ProtoMessage() {}

func (x *AnalyzeByIDResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[52]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AnalyzeByIDResponse.ProtoReflect.Descriptor instead.
func (*AnalyzeByIDResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{52}
}

func (x *AnalyzeByIDResponse) GetResponseId() string {
	if x != nil {
		return x.ResponseId
	}
	return ""
}

func (x *AnalyzeByIDResponse) GetContentId() string {
	if x != nil {
		return x.ContentId
	}
	return ""
}

func (x *AnalyzeByIDResponse) GetAnalysisType() AnalysisType {
	if x != nil {
		return x.AnalysisType
	}
	return AnalysisType_ANALYSIS_TYPE_UNSPECIFIED
}

func (x *AnalyzeByIDResponse) GetContentType() string {
	if x != nil {
		return x.ContentType
	}
	return ""
}

func (x *AnalyzeByIDResponse) GetSummary() string {
	if x != nil {
		return x.Summary
	}
	return ""
}

func (x *AnalyzeByIDResponse) GetSentiment() *SentimentResult {
	if x != nil {
		return x.Sentiment
	}
	return nil
}

func (x *AnalyzeByIDResponse) GetEntities() []*ExtractedEntity {
	if x != nil {
		return x.Entities
	}
	return nil
}

func (x *AnalyzeByIDResponse) GetTopics() []*TopicResult {
	if x != nil {
		return x.Topics
	}
	return nil
}

func (x *AnalyzeByIDResponse) GetActionItems() []*ActionItem {
	if x != nil {
		return x.ActionItems
	}
	return nil
}

func (x *AnalyzeByIDResponse) GetInsights() []string {
	if x != nil {
		return x.Insights
	}
	return nil
}

func (x *AnalyzeByIDResponse) GetModelUsed() string {
	if x != nil {
		return x.ModelUsed
	}
	return ""
}

func (x *AnalyzeByIDResponse) GetInputTokens() int32 {
	if x != nil && x.InputTokens != nil {
		return *x.InputTokens
	}
	return 0
}

func (x *AnalyzeByIDResponse) GetOutputTokens() int32 {
	if x != nil && x.OutputTokens != nil {
		return *x.OutputTokens
	}
	return 0
}

func (x *AnalyzeByIDResponse) GetLatencyMs() float64 {
	if x != nil && x.LatencyMs != nil {
		return *x.LatencyMs
	}
	return 0
}

// StageModelConfig describes the model configuration for a single pipeline stage.
type StageModelConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Pipeline stage name (e.g. "triage", "extract_semantic", "deep_analyze", "embedding").
	Stage string `protobuf:"bytes,1,opt,name=stage,proto3" json:"stage,omitempty"`
	// The model currently configured for this stage.
	Model string `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	// Where the config came from: "db", "env", "default".
	Source string `protobuf:"bytes,3,opt,name=source,proto3" json:"source,omitempty"`
	// The env var name that would apply if no DB override (e.g. "AI_MODEL_TRIAGE").
	EnvVar string `protobuf:"bytes,4,opt,name=env_var,json=envVar,proto3" json:"env_var,omitempty"`
	// The backend provider: "ollama", "gemini", "openai", "anthropic".
	Backend       string `protobuf:"bytes,5,opt,name=backend,proto3" json:"backend,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StageModelConfig) Reset() {
	*x = StageModelConfig{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[53]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StageModelConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StageModelConfig) ProtoMessage() {}

func (x *StageModelConfig) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[53]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StageModelConfig.ProtoReflect.Descriptor instead.
func (*StageModelConfig) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{53}
}

func (x *StageModelConfig) GetStage() string {
	if x != nil {
		return x.Stage
	}
	return ""
}

func (x *StageModelConfig) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *StageModelConfig) GetSource() string {
	if x != nil {
		return x.Source
	}
	return ""
}

func (x *StageModelConfig) GetEnvVar() string {
	if x != nil {
		return x.EnvVar
	}
	return ""
}

func (x *StageModelConfig) GetBackend() string {
	if x != nil {
		return x.Backend
	}
	return ""
}

// GetStageConfigRequest requests the current stagemodel mapping.
type GetStageConfigRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional: filter to a specific stage. If empty, returns all stages.
	Stage         *string `protobuf:"bytes,1,opt,name=stage,proto3,oneof" json:"stage,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetStageConfigRequest) Reset() {
	*x = GetStageConfigRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[54]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetStageConfigRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetStageConfigRequest) ProtoMessage() {}

func (x *GetStageConfigRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[54]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetStageConfigRequest.ProtoReflect.Descriptor instead.
func (*GetStageConfigRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{54}
}

func (x *GetStageConfigRequest) GetStage() string {
	if x != nil && x.Stage != nil {
		return *x.Stage
	}
	return ""
}

// GetStageConfigResponse returns the model configuration for pipeline stages.
type GetStageConfigResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Per-stage model configuration.
	Stages []*StageModelConfig `protobuf:"bytes,1,rep,name=stages,proto3" json:"stages,omitempty"`
	// Default LLM model.
	DefaultLlm *StageModelConfig `protobuf:"bytes,2,opt,name=default_llm,json=defaultLlm,proto3" json:"default_llm,omitempty"`
	// Default embedding model.
	DefaultEmbedding *StageModelConfig `protobuf:"bytes,3,opt,name=default_embedding,json=defaultEmbedding,proto3" json:"default_embedding,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *GetStageConfigResponse) Reset() {
	*x = GetStageConfigResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[55]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetStageConfigResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetStageConfigResponse) ProtoMessage() {}

func (x *GetStageConfigResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[55]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetStageConfigResponse.ProtoReflect.Descriptor instead.
func (*GetStageConfigResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{55}
}

func (x *GetStageConfigResponse) GetStages() []*StageModelConfig {
	if x != nil {
		return x.Stages
	}
	return nil
}

func (x *GetStageConfigResponse) GetDefaultLlm() *StageModelConfig {
	if x != nil {
		return x.DefaultLlm
	}
	return nil
}

func (x *GetStageConfigResponse) GetDefaultEmbedding() *StageModelConfig {
	if x != nil {
		return x.DefaultEmbedding
	}
	return nil
}

// SetStageConfigRequest sets the model for a stage or default.
type SetStageConfigRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The config key: stage name (e.g. "triage") or "default" / "default-embedding".
	Key string `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
	// The model to assign (e.g. "qwen3:8b", "gemini-2.5-pro").
	Model         string `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SetStageConfigRequest) Reset() {
	*x = SetStageConfigRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[56]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SetStageConfigRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SetStageConfigRequest) ProtoMessage() {}

func (x *SetStageConfigRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[56]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SetStageConfigRequest.ProtoReflect.Descriptor instead.
func (*SetStageConfigRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{56}
}

func (x *SetStageConfigRequest) GetKey() string {
	if x != nil {
		return x.Key
	}
	return ""
}

func (x *SetStageConfigRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

// SetStageConfigResponse confirms the configuration change.
type SetStageConfigResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The updated stage configuration.
	Config *StageModelConfig `protobuf:"bytes,1,opt,name=config,proto3" json:"config,omitempty"`
	// The previous model that was configured.
	PreviousModel string `protobuf:"bytes,2,opt,name=previous_model,json=previousModel,proto3" json:"previous_model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SetStageConfigResponse) Reset() {
	*x = SetStageConfigResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[57]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SetStageConfigResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SetStageConfigResponse) ProtoMessage() {}

func (x *SetStageConfigResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[57]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SetStageConfigResponse.ProtoReflect.Descriptor instead.
func (*SetStageConfigResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{57}
}

func (x *SetStageConfigResponse) GetConfig() *StageModelConfig {
	if x != nil {
		return x.Config
	}
	return nil
}

func (x *SetStageConfigResponse) GetPreviousModel() string {
	if x != nil {
		return x.PreviousModel
	}
	return ""
}

// ResetStageConfigRequest removes a DB override for a stage.
type ResetStageConfigRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The config key to reset: stage name or "default" / "default-embedding".
	Key           string `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ResetStageConfigRequest) Reset() {
	*x = ResetStageConfigRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[58]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ResetStageConfigRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ResetStageConfigRequest) ProtoMessage() {}

func (x *ResetStageConfigRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[58]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ResetStageConfigRequest.ProtoReflect.Descriptor instead.
func (*ResetStageConfigRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{58}
}

func (x *ResetStageConfigRequest) GetKey() string {
	if x != nil {
		return x.Key
	}
	return ""
}

// ResetStageConfigResponse confirms the reset.
type ResetStageConfigResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The effective configuration after reset (falls back to env/default).
	Config        *StageModelConfig `protobuf:"bytes,1,opt,name=config,proto3" json:"config,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ResetStageConfigResponse) Reset() {
	*x = ResetStageConfigResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[59]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ResetStageConfigResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ResetStageConfigResponse) ProtoMessage() {}

func (x *ResetStageConfigResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[59]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ResetStageConfigResponse.ProtoReflect.Descriptor instead.
func (*ResetStageConfigResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{59}
}

func (x *ResetStageConfigResponse) GetConfig() *StageModelConfig {
	if x != nil {
		return x.Config
	}
	return nil
}

// AvailableModel describes a model available for use.
type AvailableModel struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Model name/identifier.
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// Backend provider: "ollama", "gemini", "openai", "anthropic".
	Backend string `protobuf:"bytes,2,opt,name=backend,proto3" json:"backend,omitempty"`
	// Model size (for local models, e.g. "4.7 GB").
	Size *string `protobuf:"bytes,3,opt,name=size,proto3,oneof" json:"size,omitempty"`
	// Whether this model is currently assigned to any stage.
	InUse bool `protobuf:"varint,4,opt,name=in_use,json=inUse,proto3" json:"in_use,omitempty"`
	// Stages using this model (if in_use is true).
	UsedByStages  []string `protobuf:"bytes,5,rep,name=used_by_stages,json=usedByStages,proto3" json:"used_by_stages,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AvailableModel) Reset() {
	*x = AvailableModel{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[60]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AvailableModel) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AvailableModel) ProtoMessage() {}

func (x *AvailableModel) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[60]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AvailableModel.ProtoReflect.Descriptor instead.
func (*AvailableModel) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{60}
}

func (x *AvailableModel) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *AvailableModel) GetBackend() string {
	if x != nil {
		return x.Backend
	}
	return ""
}

func (x *AvailableModel) GetSize() string {
	if x != nil && x.Size != nil {
		return *x.Size
	}
	return ""
}

func (x *AvailableModel) GetInUse() bool {
	if x != nil {
		return x.InUse
	}
	return false
}

func (x *AvailableModel) GetUsedByStages() []string {
	if x != nil {
		return x.UsedByStages
	}
	return nil
}

// ListAvailableModelsRequest lists models available for assignment.
type ListAvailableModelsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional filter by backend: "ollama", "gemini", "openai", "anthropic".
	Backend       *string `protobuf:"bytes,1,opt,name=backend,proto3,oneof" json:"backend,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListAvailableModelsRequest) Reset() {
	*x = ListAvailableModelsRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[61]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListAvailableModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListAvailableModelsRequest) ProtoMessage() {}

func (x *ListAvailableModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[61]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListAvailableModelsRequest.ProtoReflect.Descriptor instead.
func (*ListAvailableModelsRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{61}
}

func (x *ListAvailableModelsRequest) GetBackend() string {
	if x != nil && x.Backend != nil {
		return *x.Backend
	}
	return ""
}

// ListAvailableModelsResponse returns available models.
type ListAvailableModelsResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Available models from all backends.
	Models        []*AvailableModel `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListAvailableModelsResponse) Reset() {
	*x = ListAvailableModelsResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[62]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListAvailableModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListAvailableModelsResponse) ProtoMessage() {}

func (x *ListAvailableModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[62]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListAvailableModelsResponse.ProtoReflect.Descriptor instead.
func (*ListAvailableModelsResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{62}
}

func (x *ListAvailableModelsResponse) GetModels() []*AvailableModel {
	if x != nil {
		return x.Models
	}
	return nil
}

// TestStageRequest runs a quick inference test on a pipeline stage.
type TestStageRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The pipeline stage to test (e.g. "triage", "extract_semantic").
	Stage         string `protobuf:"bytes,1,opt,name=stage,proto3" json:"stage,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TestStageRequest) Reset() {
	*x = TestStageRequest{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[63]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TestStageRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TestStageRequest) ProtoMessage() {}

func (x *TestStageRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[63]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TestStageRequest.ProtoReflect.Descriptor instead.
func (*TestStageRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{63}
}

func (x *TestStageRequest) GetStage() string {
	if x != nil {
		return x.Stage
	}
	return ""
}

// TestStageResponse returns the test results.
type TestStageResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The stage that was tested.
	Stage string `protobuf:"bytes,1,opt,name=stage,proto3" json:"stage,omitempty"`
	// The model used for the test.
	Model string `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	// The backend provider.
	Backend string `protobuf:"bytes,3,opt,name=backend,proto3" json:"backend,omitempty"`
	// Inference latency in milliseconds.
	LatencyMs float64 `protobuf:"fixed64,4,opt,name=latency_ms,json=latencyMs,proto3" json:"latency_ms,omitempty"`
	// Whether the test succeeded.
	Success bool `protobuf:"varint,5,opt,name=success,proto3" json:"success,omitempty"`
	// Error message if the test failed.
	Error *string `protobuf:"bytes,6,opt,name=error,proto3,oneof" json:"error,omitempty"`
	// Truncated output from the test inference.
	OutputPreview *string `protobuf:"bytes,7,opt,name=output_preview,json=outputPreview,proto3,oneof" json:"output_preview,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TestStageResponse) Reset() {
	*x = TestStageResponse{}
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[64]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TestStageResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TestStageResponse) ProtoMessage() {}

func (x *TestStageResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_ai_v1_ai_proto_msgTypes[64]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TestStageResponse.ProtoReflect.Descriptor instead.
func (*TestStageResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_ai_v1_ai_proto_rawDescGZIP(), []int{64}
}

func (x *TestStageResponse) GetStage() string {
	if x != nil {
		return x.Stage
	}
	return ""
}

func (x *TestStageResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TestStageResponse) GetBackend() string {
	if x != nil {
		return x.Backend
	}
	return ""
}

func (x *TestStageResponse) GetLatencyMs() float64 {
	if x != nil {
		return x.LatencyMs
	}
	return 0
}

func (x *TestStageResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *TestStageResponse) GetError() string {
	if x != nil && x.Error != nil {
		return *x.Error
	}
	return ""
}

func (x *TestStageResponse) GetOutputPreview() string {
	if x != nil && x.OutputPreview != nil {
		return *x.OutputPreview
	}
	return ""
}

var File_api_proto_ai_v1_ai_proto protoreflect.FileDescriptor

const file_api_proto_ai_v1_ai_proto_rawDesc = "" +
	"\n" +
	"\x18api/proto/ai/v1/ai.proto\x12\rpenfold.ai.v1\"\xb9\x02\n" +
	"\x10EmbeddingRequest\x12\x12\n" +
	"\x04text\x18\x01 \x01(\tR\x04text\x12\x19\n" +
	"\x05model\x18\x02 \x01(\tH\x00R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\x03 \x01(\tH\x01R\btenantId\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\x04 \x01(\tH\x02R\x0fpipelineTraceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\x05 \x01(\tH\x03R\tcontentId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\x06 \x01(\tH\x04R\x0epipelineSpanId\x88\x01\x01B\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\x14\n" +
	"\x12_pipeline_trace_idB\r\n" +
	"\v_content_idB\x13\n" +
	"\x11_pipeline_span_id\"\xa0\x01\n" +
	"\x11EmbeddingResponse\x12\x16\n" +
	"\x06vector\x18\x01 \x03(\x02R\x06vector\x12\x1e\n" +
	"\n" +
	"dimensions\x18\x02 \x01(\x05R\n" +
	"dimensions\x12\x1d\n" +
	"\n" +
	"model_used\x18\x03 \x01(\tR\tmodelUsed\x12$\n" +
	"\vtoken_count\x18\x04 \x01(\x05H\x00R\n" +
	"tokenCount\x88\x01\x01B\x0e\n" +
	"\f_token_count\"\xd3\x03\n" +
	"\x0eSummaryRequest\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\x12\"\n" +
	"\n" +
	"max_length\x18\x02 \x01(\x05H\x00R\tmaxLength\x88\x01\x01\x121\n" +
	"\x05style\x18\x03 \x01(\x0e2\x1b.penfold.ai.v1.SummaryStyleR\x05style\x12\x19\n" +
	"\x05model\x18\x04 \x01(\tH\x01R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\x05 \x01(\tH\x02R\btenantId\x88\x01\x01\x12 \n" +
	"\tjson_mode\x18\x06 \x01(\bH\x03R\bjsonMode\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\a \x01(\tH\x04R\x0fpipelineTraceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\b \x01(\tH\x05R\tcontentId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\t \x01(\tH\x06R\x0epipelineSpanId\x88\x01\x01B\r\n" +
	"\v_max_lengthB\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\f\n" +
	"\n" +
	"_json_modeB\x14\n" +
	"\x12_pipeline_trace_idB\r\n" +
	"\v_content_idB\x13\n" +
	"\x11_pipeline_span_id\"\x9a\x02\n" +
	"\x0fSummaryResponse\x12\x18\n" +
	"\asummary\x18\x01 \x01(\tR\asummary\x12\x1d\n" +
	"\n" +
	"key_points\x18\x02 \x03(\tR\tkeyPoints\x12\x1d\n" +
	"\n" +
	"model_used\x18\x03 \x01(\tR\tmodelUsed\x12&\n" +
	"\finput_tokens\x18\x04 \x01(\x05H\x00R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\x05 \x01(\x05H\x01R\foutputTokens\x88\x01\x01\x12(\n" +
	"\rfinish_reason\x18\x06 \x01(\tH\x02R\ffinishReason\x88\x01\x01B\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokensB\x10\n" +
	"\x0e_finish_reason\"\xbd\x03\n" +
	"\x10AssertionRequest\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\x12*\n" +
	"\x0emin_confidence\x18\x02 \x01(\x02H\x00R\rminConfidence\x88\x01\x01\x12*\n" +
	"\x0emax_assertions\x18\x03 \x01(\x05H\x01R\rmaxAssertions\x88\x01\x01\x12\x19\n" +
	"\x05model\x18\x04 \x01(\tH\x02R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\x05 \x01(\tH\x03R\btenantId\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\x06 \x01(\tH\x04R\x0fpipelineTraceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\a \x01(\tH\x05R\tcontentId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\b \x01(\tH\x06R\x0epipelineSpanId\x88\x01\x01B\x11\n" +
	"\x0f_min_confidenceB\x11\n" +
	"\x0f_max_assertionsB\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\x14\n" +
	"\x12_pipeline_trace_idB\r\n" +
	"\v_content_idB\x13\n" +
	"\x11_pipeline_span_id\"\xdf\x01\n" +
	"\tAssertion\x12\x18\n" +
	"\asubject\x18\x01 \x01(\tR\asubject\x12\x1c\n" +
	"\tpredicate\x18\x02 \x01(\tR\tpredicate\x12\x16\n" +
	"\x06object\x18\x03 \x01(\tR\x06object\x12\x1e\n" +
	"\n" +
	"confidence\x18\x04 \x01(\x02R\n" +
	"confidence\x12$\n" +
	"\vsource_text\x18\x05 \x01(\tH\x00R\n" +
	"sourceText\x88\x01\x01\x12\x1f\n" +
	"\bcategory\x18\x06 \x01(\tH\x01R\bcategory\x88\x01\x01B\x0e\n" +
	"\f_source_textB\v\n" +
	"\t_category\"\xb4\x01\n" +
	"\x11AssertionResponse\x128\n" +
	"\n" +
	"assertions\x18\x01 \x03(\v2\x18.penfold.ai.v1.AssertionR\n" +
	"assertions\x12\x1d\n" +
	"\n" +
	"model_used\x18\x02 \x01(\tR\tmodelUsed\x12\x1f\n" +
	"\vtotal_found\x18\x03 \x01(\x05R\n" +
	"totalFound\x12%\n" +
	"\x0efiltered_count\x18\x04 \x01(\x05R\rfilteredCount\"\xda\x03\n" +
	"\x16ClassifyContentRequest\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\x12\x1e\n" +
	"\n" +
	"categories\x18\x02 \x03(\tR\n" +
	"categories\x12$\n" +
	"\vmulti_label\x18\x03 \x01(\bH\x00R\n" +
	"multiLabel\x88\x01\x01\x12*\n" +
	"\x0emin_confidence\x18\x04 \x01(\x02H\x01R\rminConfidence\x88\x01\x01\x12\x19\n" +
	"\x05model\x18\x05 \x01(\tH\x02R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\x06 \x01(\tH\x03R\btenantId\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\a \x01(\tH\x04R\x0fpipelineTraceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\b \x01(\tH\x05R\tcontentId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\t \x01(\tH\x06R\x0epipelineSpanId\x88\x01\x01B\x0e\n" +
	"\f_multi_labelB\x11\n" +
	"\x0f_min_confidenceB\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\x14\n" +
	"\x12_pipeline_trace_idB\r\n" +
	"\v_content_idB\x13\n" +
	"\x11_pipeline_span_id\"}\n" +
	"\x0eClassification\x12\x14\n" +
	"\x05label\x18\x01 \x01(\tR\x05label\x12\x1e\n" +
	"\n" +
	"confidence\x18\x02 \x01(\x02R\n" +
	"confidence\x12%\n" +
	"\vexplanation\x18\x03 \x01(\tH\x00R\vexplanation\x88\x01\x01B\x0e\n" +
	"\f_explanation\"\xba\x01\n" +
	"\x17ClassifyContentResponse\x12G\n" +
	"\x0fclassifications\x18\x01 \x03(\v2\x1d.penfold.ai.v1.ClassificationR\x0fclassifications\x127\n" +
	"\aprimary\x18\x02 \x01(\v2\x1d.penfold.ai.v1.ClassificationR\aprimary\x12\x1d\n" +
	"\n" +
	"model_used\x18\x03 \x01(\tR\tmodelUsed\"\xc6\x03\n" +
	"\x14TriageContentRequest\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\x12\x1d\n" +
	"\asubject\x18\x02 \x01(\tH\x00R\asubject\x88\x01\x01\x12\x1b\n" +
	"\x06sender\x18\x03 \x01(\tH\x01R\x06sender\x88\x01\x01\x12\x19\n" +
	"\x05model\x18\x04 \x01(\tH\x02R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\x05 \x01(\tH\x03R\btenantId\x88\x01\x01\x12 \n" +
	"\tsource_id\x18\x06 \x01(\x03H\x04R\bsourceId\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\a \x01(\tH\x05R\x0fpipelineTraceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\b \x01(\tH\x06R\tcontentId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\t \x01(\tH\aR\x0epipelineSpanId\x88\x01\x01B\n" +
	"\n" +
	"\b_subjectB\t\n" +
	"\a_senderB\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\f\n" +
	"\n" +
	"_source_idB\x14\n" +
	"\x12_pipeline_trace_idB\r\n" +
	"\v_content_idB\x13\n" +
	"\x11_pipeline_span_id\"\xb8\x03\n" +
	"\x15TriageContentResponse\x12\x1a\n" +
	"\bcategory\x18\x01 \x01(\tR\bcategory\x12\x1e\n" +
	"\n" +
	"importance\x18\x02 \x01(\tR\n" +
	"importance\x12\x16\n" +
	"\x06reason\x18\x03 \x01(\tR\x06reason\x12\x1d\n" +
	"\n" +
	"model_used\x18\x04 \x01(\tR\tmodelUsed\x12&\n" +
	"\finput_tokens\x18\x05 \x01(\x05H\x00R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\x06 \x01(\x05H\x01R\foutputTokens\x88\x01\x01\x12\x18\n" +
	"\aretries\x18\a \x01(\x05R\aretries\x126\n" +
	"\x14content_contribution\x18\b \x01(\tH\x02R\x13contentContribution\x88\x01\x01\x124\n" +
	"\x13contribution_reason\x18\t \x01(\tH\x03R\x12contributionReason\x88\x01\x01B\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokensB\x17\n" +
	"\x15_content_contributionB\x16\n" +
	"\x14_contribution_reason\"\xb7\x03\n" +
	"\x16ExtractEntitiesRequest\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\x12,\n" +
	"\x0ftriage_category\x18\x02 \x01(\tH\x00R\x0etriageCategory\x88\x01\x01\x12\x19\n" +
	"\x05model\x18\x03 \x01(\tH\x01R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\x04 \x01(\tH\x02R\btenantId\x88\x01\x01\x12 \n" +
	"\tsource_id\x18\x05 \x01(\x03H\x03R\bsourceId\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\x06 \x01(\tH\x04R\x0fpipelineTraceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\a \x01(\tH\x05R\tcontentId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\b \x01(\tH\x06R\x0epipelineSpanId\x88\x01\x01B\x12\n" +
	"\x10_triage_categoryB\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\f\n" +
	"\n" +
	"_source_idB\x14\n" +
	"\x12_pipeline_trace_idB\r\n" +
	"\v_content_idB\x13\n" +
	"\x11_pipeline_span_id\"6\n" +
	"\fPersonEntity\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x12\n" +
	"\x04role\x18\x02 \x01(\tR\x04role\":\n" +
	"\n" +
	"DateEntity\x12\x12\n" +
	"\x04date\x18\x01 \x01(\tR\x04date\x12\x18\n" +
	"\acontext\x18\x02 \x01(\tR\acontext\"X\n" +
	"\x10ActionItemEntity\x12\x1a\n" +
	"\bassignee\x18\x01 \x01(\tR\bassignee\x12\x16\n" +
	"\x06action\x18\x02 \x01(\tR\x06action\x12\x10\n" +
	"\x03due\x18\x03 \x01(\tR\x03due\"r\n" +
	"\n" +
	"RiskEntity\x12 \n" +
	"\vdescription\x18\x01 \x01(\tR\vdescription\x12#\n" +
	"\rseverity_hint\x18\x02 \x01(\tR\fseverityHint\x12\x1d\n" +
	"\n" +
	"owner_hint\x18\x03 \x01(\tR\townerHint\"\xdf\x04\n" +
	"\x17ExtractEntitiesResponse\x123\n" +
	"\x06people\x18\x01 \x03(\v2\x1b.penfold.ai.v1.PersonEntityR\x06people\x12/\n" +
	"\x05dates\x18\x02 \x03(\v2\x19.penfold.ai.v1.DateEntityR\x05dates\x12\x1a\n" +
	"\bprojects\x18\x03 \x03(\tR\bprojects\x12$\n" +
	"\rorganisations\x18\x04 \x03(\tR\rorganisations\x12B\n" +
	"\faction_items\x18\x05 \x03(\v2\x1f.penfold.ai.v1.ActionItemEntityR\vactionItems\x12\x1c\n" +
	"\tdecisions\x18\x06 \x03(\tR\tdecisions\x12\x14\n" +
	"\x05risks\x18\a \x03(\tR\x05risks\x12@\n" +
	"\x0edetailed_risks\x18\b \x03(\v2\x19.penfold.ai.v1.RiskEntityR\rdetailedRisks\x124\n" +
	"\x16quality_gate_triggered\x18\t \x01(\bR\x14qualityGateTriggered\x12\x1d\n" +
	"\n" +
	"model_used\x18\n" +
	" \x01(\tR\tmodelUsed\x12&\n" +
	"\finput_tokens\x18\v \x01(\x05H\x00R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\f \x01(\x05H\x01R\foutputTokens\x88\x01\x01\x12\x18\n" +
	"\aretries\x18\r \x01(\x05R\aretriesB\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokens\"\x9f\a\n" +
	"\x12DeepAnalyzeRequest\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\x12D\n" +
	"\x0fverified_people\x18\x02 \x03(\v2\x1b.penfold.ai.v1.PersonEntityR\x0everifiedPeople\x12@\n" +
	"\x0everified_dates\x18\x03 \x03(\v2\x19.penfold.ai.v1.DateEntityR\rverifiedDates\x12+\n" +
	"\x11verified_projects\x18\x04 \x03(\tR\x10verifiedProjects\x125\n" +
	"\x16verified_organisations\x18\x05 \x03(\tR\x15verifiedOrganisations\x12Y\n" +
	"\x18preliminary_action_items\x18\x06 \x03(\v2\x1f.penfold.ai.v1.ActionItemEntityR\x16preliminaryActionItems\x123\n" +
	"\x15preliminary_decisions\x18\a \x03(\tR\x14preliminaryDecisions\x12+\n" +
	"\x11preliminary_risks\x18\b \x03(\tR\x10preliminaryRisks\x12-\n" +
	"\x12background_context\x18\t \x01(\tR\x11backgroundContext\x12'\n" +
	"\x0ftriage_category\x18\n" +
	" \x01(\tR\x0etriageCategory\x12+\n" +
	"\x11triage_importance\x18\v \x01(\tR\x10triageImportance\x12\x19\n" +
	"\x05model\x18\f \x01(\tH\x00R\x05model\x88\x01\x01\x12 \n" +
	"\ttenant_id\x18\r \x01(\tH\x01R\btenantId\x88\x01\x01\x12 \n" +
	"\tsource_id\x18\x0e \x01(\x03H\x02R\bsourceId\x88\x01\x01\x12\"\n" +
	"\n" +
	"content_id\x18\x0f \x01(\tH\x03R\tcontentId\x88\x01\x01\x12/\n" +
	"\x11pipeline_trace_id\x18\x10 \x01(\tH\x04R\x0fpipelineTraceId\x88\x01\x01\x12-\n" +
	"\x10pipeline_span_id\x18\x11 \x01(\tH\x05R\x0epipelineSpanId\x88\x01\x01B\b\n" +
	"\x06_modelB\f\n" +
	"\n" +
	"_tenant_idB\f\n" +
	"\n" +
	"_source_idB\r\n" +
	"\v_content_idB\x14\n" +
	"\x12_pipeline_trace_idB\x13\n" +
	"\x11_pipeline_span_id\"\xb7\x05\n" +
	"\x13DeepAnalyzeResponse\x12\x18\n" +
	"\asummary\x18\x01 \x01(\tR\asummary\x12:\n" +
	"\tsentiment\x18\x02 \x01(\v2\x1c.penfold.ai.v1.DeepSentimentR\tsentiment\x12B\n" +
	"\x0etopic_mappings\x18\x03 \x03(\v2\x1b.penfold.ai.v1.TopicMappingR\rtopicMappings\x12U\n" +
	"\x15verified_action_items\x18\x04 \x03(\v2!.penfold.ai.v1.VerifiedActionItemR\x13verifiedActionItems\x12N\n" +
	"\x12verified_decisions\x18\x05 \x03(\v2\x1f.penfold.ai.v1.VerifiedDecisionR\x11verifiedDecisions\x12E\n" +
	"\x0frisk_references\x18\x06 \x03(\v2\x1c.penfold.ai.v1.RiskReferenceR\x0eriskReferences\x12-\n" +
	"\x12strategic_insights\x18\a \x03(\tR\x11strategicInsights\x12U\n" +
	"\x15implicit_action_items\x18\b \x03(\v2!.penfold.ai.v1.ImplicitActionItemR\x13implicitActionItems\x12\x1d\n" +
	"\n" +
	"model_used\x18\t \x01(\tR\tmodelUsed\x12&\n" +
	"\finput_tokens\x18\n" +
	" \x01(\x05H\x00R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\v \x01(\x05H\x01R\foutputTokens\x88\x01\x01B\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokens\"\x9d\x01\n" +
	"\rDeepSentiment\x12\x14\n" +
	"\x05score\x18\x01 \x01(\x02R\x05score\x12\x14\n" +
	"\x05label\x18\x02 \x01(\tR\x05label\x12\x1e\n" +
	"\n" +
	"confidence\x18\x03 \x01(\x02R\n" +
	"confidence\x12\x1e\n" +
	"\n" +
	"indicators\x18\x04 \x03(\tR\n" +
	"indicators\x12 \n" +
	"\vexplanation\x18\x05 \x01(\tR\vexplanation\"\x91\x01\n" +
	"\fTopicMapping\x12\x14\n" +
	"\x05topic\x18\x01 \x01(\tR\x05topic\x12'\n" +
	"\x0frelated_project\x18\x02 \x01(\tR\x0erelatedProject\x12\"\n" +
	"\frelationship\x18\x03 \x01(\tR\frelationship\x12\x1e\n" +
	"\n" +
	"confidence\x18\x04 \x01(\x02R\n" +
	"confidence\"\xc1\x01\n" +
	"\x12VerifiedActionItem\x12 \n" +
	"\vdescription\x18\x01 \x01(\tR\vdescription\x12\x1a\n" +
	"\bassignee\x18\x02 \x01(\tR\bassignee\x12\x10\n" +
	"\x03due\x18\x03 \x01(\tR\x03due\x12\x1a\n" +
	"\bpriority\x18\x04 \x01(\tR\bpriority\x12'\n" +
	"\x0fcontext_excerpt\x18\x05 \x01(\tR\x0econtextExcerpt\x12\x16\n" +
	"\x06status\x18\x06 \x01(\tR\x06status\"u\n" +
	"\x10VerifiedDecision\x12 \n" +
	"\vdescription\x18\x01 \x01(\tR\vdescription\x12'\n" +
	"\x0fcontext_excerpt\x18\x02 \x01(\tR\x0econtextExcerpt\x12\x16\n" +
	"\x06status\x18\x03 \x01(\tR\x06status\"\xff\x02\n" +
	"\rRiskReference\x12\x1c\n" +
	"\aroot_id\x18\x01 \x01(\x03H\x00R\x06rootId\x88\x01\x01\x12 \n" +
	"\vdescription\x18\x02 \x01(\tR\vdescription\x12.\n" +
	"\x10lifecycle_change\x18\x03 \x01(\tH\x01R\x0flifecycleChange\x88\x01\x01\x12\"\n" +
	"\fsignificance\x18\x04 \x01(\tR\fsignificance\x12'\n" +
	"\x0fcontext_excerpt\x18\x05 \x01(\tR\x0econtextExcerpt\x12,\n" +
	"\x0fseverity_change\x18\x06 \x01(\tH\x02R\x0eseverityChange\x88\x01\x01\x12&\n" +
	"\fowner_change\x18\a \x01(\tH\x03R\vownerChange\x88\x01\x01\x12\x15\n" +
	"\x06is_new\x18\b \x01(\bR\x05isNewB\n" +
	"\n" +
	"\b_root_idB\x13\n" +
	"\x11_lifecycle_changeB\x12\n" +
	"\x10_severity_changeB\x0f\n" +
	"\r_owner_change\"}\n" +
	"\x12ImplicitActionItem\x12 \n" +
	"\vdescription\x18\x01 \x01(\tR\vdescription\x12\x1c\n" +
	"\treasoning\x18\x02 \x01(\tR\treasoning\x12'\n" +
	"\x0fcontext_excerpt\x18\x03 \x01(\tR\x0econtextExcerpt\"\x8d\a\n" +
	"\tModelInfo\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12,\n" +
	"\x04type\x18\x03 \x01(\x0e2\x18.penfold.ai.v1.ModelTypeR\x04type\x122\n" +
	"\x06status\x18\x04 \x01(\x0e2\x1a.penfold.ai.v1.ModelStatusR\x06status\x12\"\n" +
	"\fcapabilities\x18\x05 \x03(\tR\fcapabilities\x121\n" +
	"\x12max_context_length\x18\x06 \x01(\x05H\x00R\x10maxContextLength\x88\x01\x01\x126\n" +
	"\x14embedding_dimensions\x18\a \x01(\x05H\x01R\x13embeddingDimensions\x88\x01\x01\x12\x19\n" +
	"\bis_local\x18\b \x01(\bR\aisLocal\x12\x1a\n" +
	"\bprovider\x18\t \x01(\tR\bprovider\x12\x1d\n" +
	"\n" +
	"model_name\x18\n" +
	" \x01(\tR\tmodelName\x12\x1d\n" +
	"\aversion\x18\v \x01(\tH\x02R\aversion\x88\x01\x01\x12(\n" +
	"\rerror_message\x18\f \x01(\tH\x03R\ferrorMessage\x88\x01\x01\x12)\n" +
	"\x0eavg_latency_ms\x18\r \x01(\x01H\x04R\favgLatencyMs\x88\x01\x01\x121\n" +
	"\x12requests_last_hour\x18\x0e \x01(\x03H\x05R\x10requestsLastHour\x88\x01\x01\x12\x1f\n" +
	"\bendpoint\x18\x0f \x01(\tH\x06R\bendpoint\x88\x01\x01\x12.\n" +
	"\x11input_cost_per_1k\x18\x10 \x01(\x01H\aR\x0einputCostPer1k\x88\x01\x01\x120\n" +
	"\x12output_cost_per_1k\x18\x11 \x01(\x01H\bR\x0foutputCostPer1k\x88\x01\x01\x12\x1a\n" +
	"\bpriority\x18\x12 \x01(\x05R\bpriority\x12\x1d\n" +
	"\n" +
	"is_enabled\x18\x13 \x01(\bR\tisEnabledB\x15\n" +
	"\x13_max_context_lengthB\x17\n" +
	"\x15_embedding_dimensionsB\n" +
	"\n" +
	"\b_versionB\x10\n" +
	"\x0e_error_messageB\x11\n" +
	"\x0f_avg_latency_msB\x15\n" +
	"\x13_requests_last_hourB\v\n" +
	"\t_endpointB\x14\n" +
	"\x12_input_cost_per_1kB\x15\n" +
	"\x13_output_cost_per_1k\"\xd9\x01\n" +
	"\x15GetModelStatusRequest\x12\"\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tH\x00R\tmodelName\x88\x01\x01\x12<\n" +
	"\n" +
	"model_type\x18\x02 \x01(\x0e2\x18.penfold.ai.v1.ModelTypeH\x01R\tmodelType\x88\x01\x01\x12,\n" +
	"\x0finclude_metrics\x18\x03 \x01(\bH\x02R\x0eincludeMetrics\x88\x01\x01B\r\n" +
	"\v_model_nameB\r\n" +
	"\v_model_typeB\x12\n" +
	"\x10_include_metrics\"\xe2\x01\n" +
	"\x16GetModelStatusResponse\x120\n" +
	"\x06models\x18\x01 \x03(\v2\x18.penfold.ai.v1.ModelInfoR\x06models\x126\n" +
	"\x17default_embedding_model\x18\x02 \x01(\tR\x15defaultEmbeddingModel\x12*\n" +
	"\x11default_llm_model\x18\x03 \x01(\tR\x0fdefaultLlmModel\x12\x18\n" +
	"\ahealthy\x18\x04 \x01(\bR\ahealthy\x12\x18\n" +
	"\amessage\x18\x05 \x01(\tR\amessage\"\x85\x03\n" +
	"\x11ListModelsRequest\x12\x1f\n" +
	"\bprovider\x18\x01 \x01(\tH\x00R\bprovider\x88\x01\x01\x12#\n" +
	"\n" +
	"capability\x18\x02 \x01(\tH\x01R\n" +
	"capability\x88\x01\x01\x12\x1e\n" +
	"\bis_local\x18\x03 \x01(\bH\x02R\aisLocal\x88\x01\x01\x12\"\n" +
	"\n" +
	"is_enabled\x18\x04 \x01(\bH\x03R\tisEnabled\x88\x01\x01\x12<\n" +
	"\n" +
	"model_type\x18\x05 \x01(\x0e2\x18.penfold.ai.v1.ModelTypeH\x04R\tmodelType\x88\x01\x01\x12 \n" +
	"\tpage_size\x18\x06 \x01(\x05H\x05R\bpageSize\x88\x01\x01\x12\"\n" +
	"\n" +
	"page_token\x18\a \x01(\tH\x06R\tpageToken\x88\x01\x01B\v\n" +
	"\t_providerB\r\n" +
	"\v_capabilityB\v\n" +
	"\t_is_localB\r\n" +
	"\v_is_enabledB\r\n" +
	"\v_model_typeB\f\n" +
	"\n" +
	"_page_sizeB\r\n" +
	"\v_page_token\"\x8f\x01\n" +
	"\x12ListModelsResponse\x120\n" +
	"\x06models\x18\x01 \x03(\v2\x18.penfold.ai.v1.ModelInfoR\x06models\x12&\n" +
	"\x0fnext_page_token\x18\x02 \x01(\tR\rnextPageToken\x12\x1f\n" +
	"\vtotal_count\x18\x03 \x01(\x05R\n" +
	"totalCount\"\x90\x05\n" +
	"\x14RegisterModelRequest\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bprovider\x18\x02 \x01(\tR\bprovider\x12\x1d\n" +
	"\n" +
	"model_name\x18\x03 \x01(\tR\tmodelName\x12,\n" +
	"\x04type\x18\x04 \x01(\x0e2\x18.penfold.ai.v1.ModelTypeR\x04type\x12\x1f\n" +
	"\bendpoint\x18\x05 \x01(\tH\x00R\bendpoint\x88\x01\x01\x12\"\n" +
	"\fcapabilities\x18\x06 \x03(\tR\fcapabilities\x121\n" +
	"\x12max_context_length\x18\a \x01(\x05H\x01R\x10maxContextLength\x88\x01\x01\x126\n" +
	"\x14embedding_dimensions\x18\b \x01(\x05H\x02R\x13embeddingDimensions\x88\x01\x01\x12\x19\n" +
	"\bis_local\x18\t \x01(\bR\aisLocal\x12\x1d\n" +
	"\aversion\x18\n" +
	" \x01(\tH\x03R\aversion\x88\x01\x01\x12.\n" +
	"\x11input_cost_per_1k\x18\v \x01(\x01H\x04R\x0einputCostPer1k\x88\x01\x01\x120\n" +
	"\x12output_cost_per_1k\x18\f \x01(\x01H\x05R\x0foutputCostPer1k\x88\x01\x01\x12\x1a\n" +
	"\bpriority\x18\r \x01(\x05R\bpriority\x12\x1d\n" +
	"\n" +
	"is_enabled\x18\x0e \x01(\bR\tisEnabledB\v\n" +
	"\t_endpointB\x15\n" +
	"\x13_max_context_lengthB\x17\n" +
	"\x15_embedding_dimensionsB\n" +
	"\n" +
	"\b_versionB\x14\n" +
	"\x12_input_cost_per_1kB\x15\n" +
	"\x13_output_cost_per_1k\"G\n" +
	"\x15RegisterModelResponse\x12.\n" +
	"\x05model\x18\x01 \x01(\v2\x18.penfold.ai.v1.ModelInfoR\x05model\"\xae\x04\n" +
	"\x12UpdateModelRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x17\n" +
	"\x04name\x18\x02 \x01(\tH\x00R\x04name\x88\x01\x01\x12\x1f\n" +
	"\bendpoint\x18\x03 \x01(\tH\x01R\bendpoint\x88\x01\x01\x12\"\n" +
	"\fcapabilities\x18\x04 \x03(\tR\fcapabilities\x121\n" +
	"\x12max_context_length\x18\x05 \x01(\x05H\x02R\x10maxContextLength\x88\x01\x01\x126\n" +
	"\x14embedding_dimensions\x18\x06 \x01(\x05H\x03R\x13embeddingDimensions\x88\x01\x01\x12.\n" +
	"\x11input_cost_per_1k\x18\a \x01(\x01H\x04R\x0einputCostPer1k\x88\x01\x01\x120\n" +
	"\x12output_cost_per_1k\x18\b \x01(\x01H\x05R\x0foutputCostPer1k\x88\x01\x01\x12\x1f\n" +
	"\bpriority\x18\t \x01(\x05H\x06R\bpriority\x88\x01\x01\x12\"\n" +
	"\n" +
	"is_enabled\x18\n" +
	" \x01(\bH\aR\tisEnabled\x88\x01\x01B\a\n" +
	"\x05_nameB\v\n" +
	"\t_endpointB\x15\n" +
	"\x13_max_context_lengthB\x17\n" +
	"\x15_embedding_dimensionsB\x14\n" +
	"\x12_input_cost_per_1kB\x15\n" +
	"\x13_output_cost_per_1kB\v\n" +
	"\t_priorityB\r\n" +
	"\v_is_enabled\"E\n" +
	"\x13UpdateModelResponse\x12.\n" +
	"\x05model\x18\x01 \x01(\v2\x18.penfold.ai.v1.ModelInfoR\x05model\"/\n" +
	"\x12DeleteModelRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\"I\n" +
	"\x13DeleteModelResponse\x12\x18\n" +
	"\adeleted\x18\x01 \x01(\bR\adeleted\x12\x18\n" +
	"\amessage\x18\x02 \x01(\tR\amessage\"\x8e\x03\n" +
	"\vRoutingRule\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12\x1b\n" +
	"\ttask_type\x18\x03 \x01(\tR\btaskType\x12.\n" +
	"\x13preferred_model_ids\x18\x04 \x03(\tR\x11preferredModelIds\x12,\n" +
	"\x12fallback_model_ids\x18\x05 \x03(\tR\x10fallbackModelIds\x12L\n" +
	"\x11optimization_mode\x18\x06 \x01(\x0e2\x1f.penfold.ai.v1.OptimizationModeR\x10optimizationMode\x12\x1d\n" +
	"\n" +
	"is_enabled\x18\a \x01(\bR\tisEnabled\x12%\n" +
	"\vdescription\x18\b \x01(\tH\x00R\vdescription\x88\x01\x01\x12\x1d\n" +
	"\n" +
	"created_at\x18\t \x01(\tR\tcreatedAt\x12\x1d\n" +
	"\n" +
	"updated_at\x18\n" +
	" \x01(\tR\tupdatedAtB\x0e\n" +
	"\f_description\"{\n" +
	"\x16GetRoutingRulesRequest\x12 \n" +
	"\ttask_type\x18\x01 \x01(\tH\x00R\btaskType\x88\x01\x01\x12\"\n" +
	"\n" +
	"is_enabled\x18\x02 \x01(\bH\x01R\tisEnabled\x88\x01\x01B\f\n" +
	"\n" +
	"_task_typeB\r\n" +
	"\v_is_enabled\"K\n" +
	"\x17GetRoutingRulesResponse\x120\n" +
	"\x05rules\x18\x01 \x03(\v2\x1a.penfold.ai.v1.RoutingRuleR\x05rules\"\xcd\x02\n" +
	"\x18UpdateRoutingRuleRequest\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1b\n" +
	"\ttask_type\x18\x02 \x01(\tR\btaskType\x12.\n" +
	"\x13preferred_model_ids\x18\x03 \x03(\tR\x11preferredModelIds\x12,\n" +
	"\x12fallback_model_ids\x18\x04 \x03(\tR\x10fallbackModelIds\x12L\n" +
	"\x11optimization_mode\x18\x05 \x01(\x0e2\x1f.penfold.ai.v1.OptimizationModeR\x10optimizationMode\x12\x1d\n" +
	"\n" +
	"is_enabled\x18\x06 \x01(\bR\tisEnabled\x12%\n" +
	"\vdescription\x18\a \x01(\tH\x00R\vdescription\x88\x01\x01B\x0e\n" +
	"\f_description\"e\n" +
	"\x19UpdateRoutingRuleResponse\x12.\n" +
	"\x04rule\x18\x01 \x01(\v2\x1a.penfold.ai.v1.RoutingRuleR\x04rule\x12\x18\n" +
	"\acreated\x18\x02 \x01(\bR\acreated\"\xa5\x02\n" +
	"\fQueryRequest\x12\x1a\n" +
	"\bquestion\x18\x01 \x01(\tR\bquestion\x12 \n" +
	"\ttenant_id\x18\x02 \x01(\tH\x00R\btenantId\x88\x01\x01\x12(\n" +
	"\rcontext_limit\x18\x03 \x01(\x05H\x01R\fcontextLimit\x88\x01\x01\x12\x19\n" +
	"\x05model\x18\x04 \x01(\tH\x02R\x05model\x88\x01\x01\x12\"\n" +
	"\n" +
	"max_tokens\x18\x05 \x01(\x05H\x03R\tmaxTokens\x88\x01\x01\x12%\n" +
	"\vtemperature\x18\x06 \x01(\x02H\x04R\vtemperature\x88\x01\x01B\f\n" +
	"\n" +
	"_tenant_idB\x10\n" +
	"\x0e_context_limitB\b\n" +
	"\x06_modelB\r\n" +
	"\v_max_tokensB\x0e\n" +
	"\f_temperature\"\xac\x01\n" +
	"\vQuerySource\x12\x1b\n" +
	"\tsource_id\x18\x01 \x01(\tR\bsourceId\x12\x14\n" +
	"\x05title\x18\x02 \x01(\tR\x05title\x12!\n" +
	"\fcontent_type\x18\x03 \x01(\tR\vcontentType\x12\x1c\n" +
	"\trelevance\x18\x04 \x01(\x02R\trelevance\x12\x1d\n" +
	"\asnippet\x18\x05 \x01(\tH\x00R\asnippet\x88\x01\x01B\n" +
	"\n" +
	"\b_snippet\"\xc5\x02\n" +
	"\rQueryResponse\x12\x1f\n" +
	"\vresponse_id\x18\x01 \x01(\tR\n" +
	"responseId\x12\x16\n" +
	"\x06answer\x18\x02 \x01(\tR\x06answer\x124\n" +
	"\asources\x18\x03 \x03(\v2\x1a.penfold.ai.v1.QuerySourceR\asources\x12\x1d\n" +
	"\n" +
	"model_used\x18\x04 \x01(\tR\tmodelUsed\x12&\n" +
	"\finput_tokens\x18\x05 \x01(\x05H\x00R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\x06 \x01(\x05H\x01R\foutputTokens\x88\x01\x01\x12\"\n" +
	"\n" +
	"latency_ms\x18\a \x01(\x01H\x02R\tlatencyMs\x88\x01\x01B\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokensB\r\n" +
	"\v_latency_ms\"\xb2\x01\n" +
	"\x14SummarizeByIDRequest\x12\x1d\n" +
	"\n" +
	"content_id\x18\x01 \x01(\tR\tcontentId\x12 \n" +
	"\ttenant_id\x18\x02 \x01(\tH\x00R\btenantId\x88\x01\x01\x12\x1b\n" +
	"\x06length\x18\x03 \x01(\tH\x01R\x06length\x88\x01\x01\x12\x19\n" +
	"\x05model\x18\x04 \x01(\tH\x02R\x05model\x88\x01\x01B\f\n" +
	"\n" +
	"_tenant_idB\t\n" +
	"\a_lengthB\b\n" +
	"\x06_model\"\xfa\x02\n" +
	"\x15SummarizeByIDResponse\x12\x1f\n" +
	"\vresponse_id\x18\x01 \x01(\tR\n" +
	"responseId\x12\x1d\n" +
	"\n" +
	"content_id\x18\x02 \x01(\tR\tcontentId\x12\x18\n" +
	"\asummary\x18\x03 \x01(\tR\asummary\x12\x1d\n" +
	"\n" +
	"key_points\x18\x04 \x03(\tR\tkeyPoints\x12\x1d\n" +
	"\n" +
	"model_used\x18\x05 \x01(\tR\tmodelUsed\x12!\n" +
	"\fcontent_type\x18\x06 \x01(\tR\vcontentType\x12&\n" +
	"\finput_tokens\x18\a \x01(\x05H\x00R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\b \x01(\x05H\x01R\foutputTokens\x88\x01\x01\x12\"\n" +
	"\n" +
	"latency_ms\x18\t \x01(\x01H\x02R\tlatencyMs\x88\x01\x01B\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokensB\r\n" +
	"\v_latency_ms\"\xca\x01\n" +
	"\x12AnalyzeByIDRequest\x12\x1d\n" +
	"\n" +
	"content_id\x18\x01 \x01(\tR\tcontentId\x12 \n" +
	"\ttenant_id\x18\x02 \x01(\tH\x00R\btenantId\x88\x01\x01\x12@\n" +
	"\ranalysis_type\x18\x03 \x01(\x0e2\x1b.penfold.ai.v1.AnalysisTypeR\fanalysisType\x12\x19\n" +
	"\x05model\x18\x04 \x01(\tH\x01R\x05model\x88\x01\x01B\f\n" +
	"\n" +
	"_tenant_idB\b\n" +
	"\x06_model\"}\n" +
	"\x0fSentimentResult\x12\x14\n" +
	"\x05score\x18\x01 \x01(\x02R\x05score\x12\x14\n" +
	"\x05label\x18\x02 \x01(\tR\x05label\x12\x1e\n" +
	"\n" +
	"confidence\x18\x03 \x01(\x02R\n" +
	"confidence\x12\x1e\n" +
	"\n" +
	"indicators\x18\x04 \x03(\tR\n" +
	"indicators\"\x8d\x01\n" +
	"\x0fExtractedEntity\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1f\n" +
	"\ventity_type\x18\x02 \x01(\tR\n" +
	"entityType\x12#\n" +
	"\rmention_count\x18\x03 \x01(\x05R\fmentionCount\x12\x17\n" +
	"\x04role\x18\x04 \x01(\tH\x00R\x04role\x88\x01\x01B\a\n" +
	"\x05_role\"_\n" +
	"\vTopicResult\x12\x14\n" +
	"\x05topic\x18\x01 \x01(\tR\x05topic\x12\x1e\n" +
	"\n" +
	"confidence\x18\x02 \x01(\x02R\n" +
	"confidence\x12\x1a\n" +
	"\bkeywords\x18\x03 \x03(\tR\bkeywords\"\xb7\x01\n" +
	"\n" +
	"ActionItem\x12 \n" +
	"\vdescription\x18\x01 \x01(\tR\vdescription\x12\x1f\n" +
	"\bpriority\x18\x02 \x01(\tH\x00R\bpriority\x88\x01\x01\x12\x1f\n" +
	"\bassignee\x18\x03 \x01(\tH\x01R\bassignee\x88\x01\x01\x12\x1e\n" +
	"\bdue_date\x18\x04 \x01(\tH\x02R\adueDate\x88\x01\x01B\v\n" +
	"\t_priorityB\v\n" +
	"\t_assigneeB\v\n" +
	"\t_due_date\"\xb6\x05\n" +
	"\x13AnalyzeByIDResponse\x12\x1f\n" +
	"\vresponse_id\x18\x01 \x01(\tR\n" +
	"responseId\x12\x1d\n" +
	"\n" +
	"content_id\x18\x02 \x01(\tR\tcontentId\x12@\n" +
	"\ranalysis_type\x18\x03 \x01(\x0e2\x1b.penfold.ai.v1.AnalysisTypeR\fanalysisType\x12!\n" +
	"\fcontent_type\x18\x04 \x01(\tR\vcontentType\x12\x18\n" +
	"\asummary\x18\x05 \x01(\tR\asummary\x12A\n" +
	"\tsentiment\x18\x06 \x01(\v2\x1e.penfold.ai.v1.SentimentResultH\x00R\tsentiment\x88\x01\x01\x12:\n" +
	"\bentities\x18\a \x03(\v2\x1e.penfold.ai.v1.ExtractedEntityR\bentities\x122\n" +
	"\x06topics\x18\b \x03(\v2\x1a.penfold.ai.v1.TopicResultR\x06topics\x12<\n" +
	"\faction_items\x18\t \x03(\v2\x19.penfold.ai.v1.ActionItemR\vactionItems\x12\x1a\n" +
	"\binsights\x18\n" +
	" \x03(\tR\binsights\x12\x1d\n" +
	"\n" +
	"model_used\x18\v \x01(\tR\tmodelUsed\x12&\n" +
	"\finput_tokens\x18\f \x01(\x05H\x01R\vinputTokens\x88\x01\x01\x12(\n" +
	"\routput_tokens\x18\r \x01(\x05H\x02R\foutputTokens\x88\x01\x01\x12\"\n" +
	"\n" +
	"latency_ms\x18\x0e \x01(\x01H\x03R\tlatencyMs\x88\x01\x01B\f\n" +
	"\n" +
	"_sentimentB\x0f\n" +
	"\r_input_tokensB\x10\n" +
	"\x0e_output_tokensB\r\n" +
	"\v_latency_ms\"\x89\x01\n" +
	"\x10StageModelConfig\x12\x14\n" +
	"\x05stage\x18\x01 \x01(\tR\x05stage\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\x12\x16\n" +
	"\x06source\x18\x03 \x01(\tR\x06source\x12\x17\n" +
	"\aenv_var\x18\x04 \x01(\tR\x06envVar\x12\x18\n" +
	"\abackend\x18\x05 \x01(\tR\abackend\"<\n" +
	"\x15GetStageConfigRequest\x12\x19\n" +
	"\x05stage\x18\x01 \x01(\tH\x00R\x05stage\x88\x01\x01B\b\n" +
	"\x06_stage\"\xe1\x01\n" +
	"\x16GetStageConfigResponse\x127\n" +
	"\x06stages\x18\x01 \x03(\v2\x1f.penfold.ai.v1.StageModelConfigR\x06stages\x12@\n" +
	"\vdefault_llm\x18\x02 \x01(\v2\x1f.penfold.ai.v1.StageModelConfigR\n" +
	"defaultLlm\x12L\n" +
	"\x11default_embedding\x18\x03 \x01(\v2\x1f.penfold.ai.v1.StageModelConfigR\x10defaultEmbedding\"?\n" +
	"\x15SetStageConfigRequest\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\"x\n" +
	"\x16SetStageConfigResponse\x127\n" +
	"\x06config\x18\x01 \x01(\v2\x1f.penfold.ai.v1.StageModelConfigR\x06config\x12%\n" +
	"\x0eprevious_model\x18\x02 \x01(\tR\rpreviousModel\"+\n" +
	"\x17ResetStageConfigRequest\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\"S\n" +
	"\x18ResetStageConfigResponse\x127\n" +
	"\x06config\x18\x01 \x01(\v2\x1f.penfold.ai.v1.StageModelConfigR\x06config\"\x9d\x01\n" +
	"\x0eAvailableModel\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\abackend\x18\x02 \x01(\tR\abackend\x12\x17\n" +
	"\x04size\x18\x03 \x01(\tH\x00R\x04size\x88\x01\x01\x12\x15\n" +
	"\x06in_use\x18\x04 \x01(\bR\x05inUse\x12$\n" +
	"\x0eused_by_stages\x18\x05 \x03(\tR\fusedByStagesB\a\n" +
	"\x05_size\"G\n" +
	"\x1aListAvailableModelsRequest\x12\x1d\n" +
	"\abackend\x18\x01 \x01(\tH\x00R\abackend\x88\x01\x01B\n" +
	"\n" +
	"\b_backend\"T\n" +
	"\x1bListAvailableModelsResponse\x125\n" +
	"\x06models\x18\x01 \x03(\v2\x1d.penfold.ai.v1.AvailableModelR\x06models\"(\n" +
	"\x10TestStageRequest\x12\x14\n" +
	"\x05stage\x18\x01 \x01(\tR\x05stage\"\xf6\x01\n" +
	"\x11TestStageResponse\x12\x14\n" +
	"\x05stage\x18\x01 \x01(\tR\x05stage\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\x12\x18\n" +
	"\abackend\x18\x03 \x01(\tR\abackend\x12\x1d\n" +
	"\n" +
	"latency_ms\x18\x04 \x01(\x01R\tlatencyMs\x12\x18\n" +
	"\asuccess\x18\x05 \x01(\bR\asuccess\x12\x19\n" +
	"\x05error\x18\x06 \x01(\tH\x00R\x05error\x88\x01\x01\x12*\n" +
	"\x0eoutput_preview\x18\a \x01(\tH\x01R\routputPreview\x88\x01\x01B\b\n" +
	"\x06_errorB\x11\n" +
	"\x0f_output_preview*\xa0\x01\n" +
	"\fSummaryStyle\x12\x1d\n" +
	"\x19SUMMARY_STYLE_UNSPECIFIED\x10\x00\x12\x17\n" +
	"\x13SUMMARY_STYLE_BRIEF\x10\x01\x12\x1a\n" +
	"\x16SUMMARY_STYLE_DETAILED\x10\x02\x12\x1f\n" +
	"\x1bSUMMARY_STYLE_BULLET_POINTS\x10\x03\x12\x1b\n" +
	"\x17SUMMARY_STYLE_TECHNICAL\x10\x04*\x84\x01\n" +
	"\tModelType\x12\x1a\n" +
	"\x16MODEL_TYPE_UNSPECIFIED\x10\x00\x12\x18\n" +
	"\x14MODEL_TYPE_EMBEDDING\x10\x01\x12\x12\n" +
	"\x0eMODEL_TYPE_LLM\x10\x02\x12\x19\n" +
	"\x15MODEL_TYPE_CLASSIFIER\x10\x03\x12\x12\n" +
	"\x0eMODEL_TYPE_NER\x10\x04*\xab\x01\n" +
	"\vModelStatus\x12\x1c\n" +
	"\x18MODEL_STATUS_UNSPECIFIED\x10\x00\x12\x16\n" +
	"\x12MODEL_STATUS_READY\x10\x01\x12\x18\n" +
	"\x14MODEL_STATUS_LOADING\x10\x02\x12\x16\n" +
	"\x12MODEL_STATUS_ERROR\x10\x03\x12\x19\n" +
	"\x15MODEL_STATUS_UNLOADED\x10\x04\x12\x19\n" +
	"\x15MODEL_STATUS_UPDATING\x10\x05*\xaf\x01\n" +
	"\x10OptimizationMode\x12!\n" +
	"\x1dOPTIMIZATION_MODE_UNSPECIFIED\x10\x00\x12\x1d\n" +
	"\x19OPTIMIZATION_MODE_LATENCY\x10\x01\x12\x1d\n" +
	"\x19OPTIMIZATION_MODE_QUALITY\x10\x02\x12\x1a\n" +
	"\x16OPTIMIZATION_MODE_COST\x10\x03\x12\x1e\n" +
	"\x1aOPTIMIZATION_MODE_BALANCED\x10\x04*\xb2\x01\n" +
	"\fAnalysisType\x12\x1d\n" +
	"\x19ANALYSIS_TYPE_UNSPECIFIED\x10\x00\x12\x1b\n" +
	"\x17ANALYSIS_TYPE_SENTIMENT\x10\x01\x12\x1a\n" +
	"\x16ANALYSIS_TYPE_ENTITIES\x10\x02\x12\x18\n" +
	"\x14ANALYSIS_TYPE_TOPICS\x10\x03\x12\x18\n" +
	"\x14ANALYSIS_TYPE_ACTION\x10\x04\x12\x16\n" +
	"\x12ANALYSIS_TYPE_FULL\x10\x052\xe9\x0f\n" +
	"\x14AICoordinatorService\x12V\n" +
	"\x11GenerateEmbedding\x12\x1f.penfold.ai.v1.EmbeddingRequest\x1a .penfold.ai.v1.EmbeddingResponse\x12P\n" +
	"\x0fGenerateSummary\x12\x1d.penfold.ai.v1.SummaryRequest\x1a\x1e.penfold.ai.v1.SummaryResponse\x12V\n" +
	"\x11ExtractAssertions\x12\x1f.penfold.ai.v1.AssertionRequest\x1a .penfold.ai.v1.AssertionResponse\x12`\n" +
	"\x0fClassifyContent\x12%.penfold.ai.v1.ClassifyContentRequest\x1a&.penfold.ai.v1.ClassifyContentResponse\x12Z\n" +
	"\rTriageContent\x12#.penfold.ai.v1.TriageContentRequest\x1a$.penfold.ai.v1.TriageContentResponse\x12`\n" +
	"\x0fExtractEntities\x12%.penfold.ai.v1.ExtractEntitiesRequest\x1a&.penfold.ai.v1.ExtractEntitiesResponse\x12T\n" +
	"\vDeepAnalyze\x12!.penfold.ai.v1.DeepAnalyzeRequest\x1a\".penfold.ai.v1.DeepAnalyzeResponse\x12]\n" +
	"\x0eGetModelStatus\x12$.penfold.ai.v1.GetModelStatusRequest\x1a%.penfold.ai.v1.GetModelStatusResponse\x12Q\n" +
	"\n" +
	"ListModels\x12 .penfold.ai.v1.ListModelsRequest\x1a!.penfold.ai.v1.ListModelsResponse\x12Z\n" +
	"\rRegisterModel\x12#.penfold.ai.v1.RegisterModelRequest\x1a$.penfold.ai.v1.RegisterModelResponse\x12T\n" +
	"\vUpdateModel\x12!.penfold.ai.v1.UpdateModelRequest\x1a\".penfold.ai.v1.UpdateModelResponse\x12T\n" +
	"\vDeleteModel\x12!.penfold.ai.v1.DeleteModelRequest\x1a\".penfold.ai.v1.DeleteModelResponse\x12`\n" +
	"\x0fGetRoutingRules\x12%.penfold.ai.v1.GetRoutingRulesRequest\x1a&.penfold.ai.v1.GetRoutingRulesResponse\x12f\n" +
	"\x11UpdateRoutingRule\x12'.penfold.ai.v1.UpdateRoutingRuleRequest\x1a(.penfold.ai.v1.UpdateRoutingRuleResponse\x12]\n" +
	"\x0eGetStageConfig\x12$.penfold.ai.v1.GetStageConfigRequest\x1a%.penfold.ai.v1.GetStageConfigResponse\x12]\n" +
	"\x0eSetStageConfig\x12$.penfold.ai.v1.SetStageConfigRequest\x1a%.penfold.ai.v1.SetStageConfigResponse\x12c\n" +
	"\x10ResetStageConfig\x12&.penfold.ai.v1.ResetStageConfigRequest\x1a'.penfold.ai.v1.ResetStageConfigResponse\x12l\n" +
	"\x13ListAvailableModels\x12).penfold.ai.v1.ListAvailableModelsRequest\x1a*.penfold.ai.v1.ListAvailableModelsResponse\x12N\n" +
	"\tTestStage\x12\x1f.penfold.ai.v1.TestStageRequest\x1a .penfold.ai.v1.TestStageResponse\x12B\n" +
	"\x05Query\x12\x1b.penfold.ai.v1.QueryRequest\x1a\x1c.penfold.ai.v1.QueryResponse\x12Z\n" +
	"\rSummarizeByID\x12#.penfold.ai.v1.SummarizeByIDRequest\x1a$.penfold.ai.v1.SummarizeByIDResponse\x12T\n" +
	"\vAnalyzeByID\x12!.penfold.ai.v1.AnalyzeByIDRequest\x1a\".penfold.ai.v1.AnalyzeByIDResponseB:Z8github.com/otherjamesbrown/penf-cli/api/proto/ai/v1;aiv1b\x06proto3"

var (
	file_api_proto_ai_v1_ai_proto_rawDescOnce sync.Once
	file_api_proto_ai_v1_ai_proto_rawDescData []byte
)

func file_api_proto_ai_v1_ai_proto_rawDescGZIP() []byte {
	file_api_proto_ai_v1_ai_proto_rawDescOnce.Do(func() {
		file_api_proto_ai_v1_ai_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_api_proto_ai_v1_ai_proto_rawDesc), len(file_api_proto_ai_v1_ai_proto_rawDesc)))
	})
	return file_api_proto_ai_v1_ai_proto_rawDescData
}

var file_api_proto_ai_v1_ai_proto_enumTypes = make([]protoimpl.EnumInfo, 5)
var file_api_proto_ai_v1_ai_proto_msgTypes = make([]protoimpl.MessageInfo, 65)
var file_api_proto_ai_v1_ai_proto_goTypes = []any{
	(SummaryStyle)(0),                   // 0: penfold.ai.v1.SummaryStyle
	(ModelType)(0),                      // 1: penfold.ai.v1.ModelType
	(ModelStatus)(0),                    // 2: penfold.ai.v1.ModelStatus
	(OptimizationMode)(0),               // 3: penfold.ai.v1.OptimizationMode
	(AnalysisType)(0),                   // 4: penfold.ai.v1.AnalysisType
	(*EmbeddingRequest)(nil),            // 5: penfold.ai.v1.EmbeddingRequest
	(*EmbeddingResponse)(nil),           // 6: penfold.ai.v1.EmbeddingResponse
	(*SummaryRequest)(nil),              // 7: penfold.ai.v1.SummaryRequest
	(*SummaryResponse)(nil),             // 8: penfold.ai.v1.SummaryResponse
	(*AssertionRequest)(nil),            // 9: penfold.ai.v1.AssertionRequest
	(*Assertion)(nil),                   // 10: penfold.ai.v1.Assertion
	(*AssertionResponse)(nil),           // 11: penfold.ai.v1.AssertionResponse
	(*ClassifyContentRequest)(nil),      // 12: penfold.ai.v1.ClassifyContentRequest
	(*Classification)(nil),              // 13: penfold.ai.v1.Classification
	(*ClassifyContentResponse)(nil),     // 14: penfold.ai.v1.ClassifyContentResponse
	(*TriageContentRequest)(nil),        // 15: penfold.ai.v1.TriageContentRequest
	(*TriageContentResponse)(nil),       // 16: penfold.ai.v1.TriageContentResponse
	(*ExtractEntitiesRequest)(nil),      // 17: penfold.ai.v1.ExtractEntitiesRequest
	(*PersonEntity)(nil),                // 18: penfold.ai.v1.PersonEntity
	(*DateEntity)(nil),                  // 19: penfold.ai.v1.DateEntity
	(*ActionItemEntity)(nil),            // 20: penfold.ai.v1.ActionItemEntity
	(*RiskEntity)(nil),                  // 21: penfold.ai.v1.RiskEntity
	(*ExtractEntitiesResponse)(nil),     // 22: penfold.ai.v1.ExtractEntitiesResponse
	(*DeepAnalyzeRequest)(nil),          // 23: penfold.ai.v1.DeepAnalyzeRequest
	(*DeepAnalyzeResponse)(nil),         // 24: penfold.ai.v1.DeepAnalyzeResponse
	(*DeepSentiment)(nil),               // 25: penfold.ai.v1.DeepSentiment
	(*TopicMapping)(nil),                // 26: penfold.ai.v1.TopicMapping
	(*VerifiedActionItem)(nil),          // 27: penfold.ai.v1.VerifiedActionItem
	(*VerifiedDecision)(nil),            // 28: penfold.ai.v1.VerifiedDecision
	(*RiskReference)(nil),               // 29: penfold.ai.v1.RiskReference
	(*ImplicitActionItem)(nil),          // 30: penfold.ai.v1.ImplicitActionItem
	(*ModelInfo)(nil),                   // 31: penfold.ai.v1.ModelInfo
	(*GetModelStatusRequest)(nil),       // 32: penfold.ai.v1.GetModelStatusRequest
	(*GetModelStatusResponse)(nil),      // 33: penfold.ai.v1.GetModelStatusResponse
	(*ListModelsRequest)(nil),           // 34: penfold.ai.v1.ListModelsRequest
	(*ListModelsResponse)(nil),          // 35: penfold.ai.v1.ListModelsResponse
	(*RegisterModelRequest)(nil),        // 36: penfold.ai.v1.RegisterModelRequest
	(*RegisterModelResponse)(nil),       // 37: penfold.ai.v1.RegisterModelResponse
	(*UpdateModelRequest)(nil),          // 38: penfold.ai.v1.UpdateModelRequest
	(*UpdateModelResponse)(nil),         // 39: penfold.ai.v1.UpdateModelResponse
	(*DeleteModelRequest)(nil),          // 40: penfold.ai.v1.DeleteModelRequest
	(*DeleteModelResponse)(nil),         // 41: penfold.ai.v1.DeleteModelResponse
	(*RoutingRule)(nil),                 // 42: penfold.ai.v1.RoutingRule
	(*GetRoutingRulesRequest)(nil),      // 43: penfold.ai.v1.GetRoutingRulesRequest
	(*GetRoutingRulesResponse)(nil),     // 44: penfold.ai.v1.GetRoutingRulesResponse
	(*UpdateRoutingRuleRequest)(nil),    // 45: penfold.ai.v1.UpdateRoutingRuleRequest
	(*UpdateRoutingRuleResponse)(nil),   // 46: penfold.ai.v1.UpdateRoutingRuleResponse
	(*QueryRequest)(nil),                // 47: penfold.ai.v1.QueryRequest
	(*QuerySource)(nil),                 // 48: penfold.ai.v1.QuerySource
	(*QueryResponse)(nil),               // 49: penfold.ai.v1.QueryResponse
	(*SummarizeByIDRequest)(nil),        // 50: penfold.ai.v1.SummarizeByIDRequest
	(*SummarizeByIDResponse)(nil),       // 51: penfold.ai.v1.SummarizeByIDResponse
	(*AnalyzeByIDRequest)(nil),          // 52: penfold.ai.v1.AnalyzeByIDRequest
	(*SentimentResult)(nil),             // 53: penfold.ai.v1.SentimentResult
	(*ExtractedEntity)(nil),             // 54: penfold.ai.v1.ExtractedEntity
	(*TopicResult)(nil),                 // 55: penfold.ai.v1.TopicResult
	(*ActionItem)(nil),                  // 56: penfold.ai.v1.ActionItem
	(*AnalyzeByIDResponse)(nil),         // 57: penfold.ai.v1.AnalyzeByIDResponse
	(*StageModelConfig)(nil),            // 58: penfold.ai.v1.StageModelConfig
	(*GetStageConfigRequest)(nil),       // 59: penfold.ai.v1.GetStageConfigRequest
	(*GetStageConfigResponse)(nil),      // 60: penfold.ai.v1.GetStageConfigResponse
	(*SetStageConfigRequest)(nil),       // 61: penfold.ai.v1.SetStageConfigRequest
	(*SetStageConfigResponse)(nil),      // 62: penfold.ai.v1.SetStageConfigResponse
	(*ResetStageConfigRequest)(nil),     // 63: penfold.ai.v1.ResetStageConfigRequest
	(*ResetStageConfigResponse)(nil),    // 64: penfold.ai.v1.ResetStageConfigResponse
	(*AvailableModel)(nil),              // 65: penfold.ai.v1.AvailableModel
	(*ListAvailableModelsRequest)(nil),  // 66: penfold.ai.v1.ListAvailableModelsRequest
	(*ListAvailableModelsResponse)(nil), // 67: penfold.ai.v1.ListAvailableModelsResponse
	(*TestStageRequest)(nil),            // 68: penfold.ai.v1.TestStageRequest
	(*TestStageResponse)(nil),           // 69: penfold.ai.v1.TestStageResponse
}
var file_api_proto_ai_v1_ai_proto_depIdxs = []int32{
	0,  // 0: penfold.ai.v1.SummaryRequest.style:type_name -> penfold.ai.v1.SummaryStyle
	10, // 1: penfold.ai.v1.AssertionResponse.assertions:type_name -> penfold.ai.v1.Assertion
	13, // 2: penfold.ai.v1.ClassifyContentResponse.classifications:type_name -> penfold.ai.v1.Classification
	13, // 3: penfold.ai.v1.ClassifyContentResponse.primary:type_name -> penfold.ai.v1.Classification
	18, // 4: penfold.ai.v1.ExtractEntitiesResponse.people:type_name -> penfold.ai.v1.PersonEntity
	19, // 5: penfold.ai.v1.ExtractEntitiesResponse.dates:type_name -> penfold.ai.v1.DateEntity
	20, // 6: penfold.ai.v1.ExtractEntitiesResponse.action_items:type_name -> penfold.ai.v1.ActionItemEntity
	21, // 7: penfold.ai.v1.ExtractEntitiesResponse.detailed_risks:type_name -> penfold.ai.v1.RiskEntity
	18, // 8: penfold.ai.v1.DeepAnalyzeRequest.verified_people:type_name -> penfold.ai.v1.PersonEntity
	19, // 9: penfold.ai.v1.DeepAnalyzeRequest.verified_dates:type_name -> penfold.ai.v1.DateEntity
	20, // 10: penfold.ai.v1.DeepAnalyzeRequest.preliminary_action_items:type_name -> penfold.ai.v1.ActionItemEntity
	25, // 11: penfold.ai.v1.DeepAnalyzeResponse.sentiment:type_name -> penfold.ai.v1.DeepSentiment
	26, // 12: penfold.ai.v1.DeepAnalyzeResponse.topic_mappings:type_name -> penfold.ai.v1.TopicMapping
	27, // 13: penfold.ai.v1.DeepAnalyzeResponse.verified_action_items:type_name -> penfold.ai.v1.VerifiedActionItem
	28, // 14: penfold.ai.v1.DeepAnalyzeResponse.verified_decisions:type_name -> penfold.ai.v1.VerifiedDecision
	29, // 15: penfold.ai.v1.DeepAnalyzeResponse.risk_references:type_name -> penfold.ai.v1.RiskReference
	30, // 16: penfold.ai.v1.DeepAnalyzeResponse.implicit_action_items:type_name -> penfold.ai.v1.ImplicitActionItem
	1,  // 17: penfold.ai.v1.ModelInfo.type:type_name -> penfold.ai.v1.ModelType
	2,  // 18: penfold.ai.v1.ModelInfo.status:type_name -> penfold.ai.v1.ModelStatus
	1,  // 19: penfold.ai.v1.GetModelStatusRequest.model_type:type_name -> penfold.ai.v1.ModelType
	31, // 20: penfold.ai.v1.GetModelStatusResponse.models:type_name -> penfold.ai.v1.ModelInfo
	1,  // 21: penfold.ai.v1.ListModelsRequest.model_type:type_name -> penfold.ai.v1.ModelType
	31, // 22: penfold.ai.v1.ListModelsResponse.models:type_name -> penfold.ai.v1.ModelInfo
	1,  // 23: penfold.ai.v1.RegisterModelRequest.type:type_name -> penfold.ai.v1.ModelType
	31, // 24: penfold.ai.v1.RegisterModelResponse.model:type_name -> penfold.ai.v1.ModelInfo
	31, // 25: penfold.ai.v1.UpdateModelResponse.model:type_name -> penfold.ai.v1.ModelInfo
	3,  // 26: penfold.ai.v1.RoutingRule.optimization_mode:type_name -> penfold.ai.v1.OptimizationMode
	42, // 27: penfold.ai.v1.GetRoutingRulesResponse.rules:type_name -> penfold.ai.v1.RoutingRule
	3,  // 28: penfold.ai.v1.UpdateRoutingRuleRequest.optimization_mode:type_name -> penfold.ai.v1.OptimizationMode
	42, // 29: penfold.ai.v1.UpdateRoutingRuleResponse.rule:type_name -> penfold.ai.v1.RoutingRule
	48, // 30: penfold.ai.v1.QueryResponse.sources:type_name -> penfold.ai.v1.QuerySource
	4,  // 31: penfold.ai.v1.AnalyzeByIDRequest.analysis_type:type_name -> penfold.ai.v1.AnalysisType
	4,  // 32: penfold.ai.v1.AnalyzeByIDResponse.analysis_type:type_name -> penfold.ai.v1.AnalysisType
	53, // 33: penfold.ai.v1.AnalyzeByIDResponse.sentiment:type_name -> penfold.ai.v1.SentimentResult
	54, // 34: penfold.ai.v1.AnalyzeByIDResponse.entities:type_name -> penfold.ai.v1.ExtractedEntity
	55, // 35: penfold.ai.v1.AnalyzeByIDResponse.topics:type_name -> penfold.ai.v1.TopicResult
	56, // 36: penfold.ai.v1.AnalyzeByIDResponse.action_items:type_name -> penfold.ai.v1.ActionItem
	58, // 37: penfold.ai.v1.GetStageConfigResponse.stages:type_name -> penfold.ai.v1.StageModelConfig
	58, // 38: penfold.ai.v1.GetStageConfigResponse.default_llm:type_name -> penfold.ai.v1.StageModelConfig
	58, // 39: penfold.ai.v1.GetStageConfigResponse.default_embedding:type_name -> penfold.ai.v1.StageModelConfig
	58, // 40: penfold.ai.v1.SetStageConfigResponse.config:type_name -> penfold.ai.v1.StageModelConfig
	58, // 41: penfold.ai.v1.ResetStageConfigResponse.config:type_name -> penfold.ai.v1.StageModelConfig
	65, // 42: penfold.ai.v1.ListAvailableModelsResponse.models:type_name -> penfold.ai.v1.AvailableModel
	5,  // 43: penfold.ai.v1.AICoordinatorService.GenerateEmbedding:input_type -> penfold.ai.v1.EmbeddingRequest
	7,  // 44: penfold.ai.v1.AICoordinatorService.GenerateSummary:input_type -> penfold.ai.v1.SummaryRequest
	9,  // 45: penfold.ai.v1.AICoordinatorService.ExtractAssertions:input_type -> penfold.ai.v1.AssertionRequest
	12, // 46: penfold.ai.v1.AICoordinatorService.ClassifyContent:input_type -> penfold.ai.v1.ClassifyContentRequest
	15, // 47: penfold.ai.v1.AICoordinatorService.TriageContent:input_type -> penfold.ai.v1.TriageContentRequest
	17, // 48: penfold.ai.v1.AICoordinatorService.ExtractEntities:input_type -> penfold.ai.v1.ExtractEntitiesRequest
	23, // 49: penfold.ai.v1.AICoordinatorService.DeepAnalyze:input_type -> penfold.ai.v1.DeepAnalyzeRequest
	32, // 50: penfold.ai.v1.AICoordinatorService.GetModelStatus:input_type -> penfold.ai.v1.GetModelStatusRequest
	34, // 51: penfold.ai.v1.AICoordinatorService.ListModels:input_type -> penfold.ai.v1.ListModelsRequest
	36, // 52: penfold.ai.v1.AICoordinatorService.RegisterModel:input_type -> penfold.ai.v1.RegisterModelRequest
	38, // 53: penfold.ai.v1.AICoordinatorService.UpdateModel:input_type -> penfold.ai.v1.UpdateModelRequest
	40, // 54: penfold.ai.v1.AICoordinatorService.DeleteModel:input_type -> penfold.ai.v1.DeleteModelRequest
	43, // 55: penfold.ai.v1.AICoordinatorService.GetRoutingRules:input_type -> penfold.ai.v1.GetRoutingRulesRequest
	45, // 56: penfold.ai.v1.AICoordinatorService.UpdateRoutingRule:input_type -> penfold.ai.v1.UpdateRoutingRuleRequest
	59, // 57: penfold.ai.v1.AICoordinatorService.GetStageConfig:input_type -> penfold.ai.v1.GetStageConfigRequest
	61, // 58: penfold.ai.v1.AICoordinatorService.SetStageConfig:input_type -> penfold.ai.v1.SetStageConfigRequest
	63, // 59: penfold.ai.v1.AICoordinatorService.ResetStageConfig:input_type -> penfold.ai.v1.ResetStageConfigRequest
	66, // 60: penfold.ai.v1.AICoordinatorService.ListAvailableModels:input_type -> penfold.ai.v1.ListAvailableModelsRequest
	68, // 61: penfold.ai.v1.AICoordinatorService.TestStage:input_type -> penfold.ai.v1.TestStageRequest
	47, // 62: penfold.ai.v1.AICoordinatorService.Query:input_type -> penfold.ai.v1.QueryRequest
	50, // 63: penfold.ai.v1.AICoordinatorService.SummarizeByID:input_type -> penfold.ai.v1.SummarizeByIDRequest
	52, // 64: penfold.ai.v1.AICoordinatorService.AnalyzeByID:input_type -> penfold.ai.v1.AnalyzeByIDRequest
	6,  // 65: penfold.ai.v1.AICoordinatorService.GenerateEmbedding:output_type -> penfold.ai.v1.EmbeddingResponse
	8,  // 66: penfold.ai.v1.AICoordinatorService.GenerateSummary:output_type -> penfold.ai.v1.SummaryResponse
	11, // 67: penfold.ai.v1.AICoordinatorService.ExtractAssertions:output_type -> penfold.ai.v1.AssertionResponse
	14, // 68: penfold.ai.v1.AICoordinatorService.ClassifyContent:output_type -> penfold.ai.v1.ClassifyContentResponse
	16, // 69: penfold.ai.v1.AICoordinatorService.TriageContent:output_type -> penfold.ai.v1.TriageContentResponse
	22, // 70: penfold.ai.v1.AICoordinatorService.ExtractEntities:output_type -> penfold.ai.v1.ExtractEntitiesResponse
	24, // 71: penfold.ai.v1.AICoordinatorService.DeepAnalyze:output_type -> penfold.ai.v1.DeepAnalyzeResponse
	33, // 72: penfold.ai.v1.AICoordinatorService.GetModelStatus:output_type -> penfold.ai.v1.GetModelStatusResponse
	35, // 73: penfold.ai.v1.AICoordinatorService.ListModels:output_type -> penfold.ai.v1.ListModelsResponse
	37, // 74: penfold.ai.v1.AICoordinatorService.RegisterModel:output_type -> penfold.ai.v1.RegisterModelResponse
	39, // 75: penfold.ai.v1.AICoordinatorService.UpdateModel:output_type -> penfold.ai.v1.UpdateModelResponse
	41, // 76: penfold.ai.v1.AICoordinatorService.DeleteModel:output_type -> penfold.ai.v1.DeleteModelResponse
	44, // 77: penfold.ai.v1.AICoordinatorService.GetRoutingRules:output_type -> penfold.ai.v1.GetRoutingRulesResponse
	46, // 78: penfold.ai.v1.AICoordinatorService.UpdateRoutingRule:output_type -> penfold.ai.v1.UpdateRoutingRuleResponse
	60, // 79: penfold.ai.v1.AICoordinatorService.GetStageConfig:output_type -> penfold.ai.v1.GetStageConfigResponse
	62, // 80: penfold.ai.v1.AICoordinatorService.SetStageConfig:output_type -> penfold.ai.v1.SetStageConfigResponse
	64, // 81: penfold.ai.v1.AICoordinatorService.ResetStageConfig:output_type -> penfold.ai.v1.ResetStageConfigResponse
	67, // 82: penfold.ai.v1.AICoordinatorService.ListAvailableModels:output_type -> penfold.ai.v1.ListAvailableModelsResponse
	69, // 83: penfold.ai.v1.AICoordinatorService.TestStage:output_type -> penfold.ai.v1.TestStageResponse
	49, // 84: penfold.ai.v1.AICoordinatorService.Query:output_type -> penfold.ai.v1.QueryResponse
	51, // 85: penfold.ai.v1.AICoordinatorService.SummarizeByID:output_type -> penfold.ai.v1.SummarizeByIDResponse
	57, // 86: penfold.ai.v1.AICoordinatorService.AnalyzeByID:output_type -> penfold.ai.v1.AnalyzeByIDResponse
	65, // [65:87] is the sub-list for method output_type
	43, // [43:65] is the sub-list for method input_type
	43, // [43:43] is the sub-list for extension type_name
	43, // [43:43] is the sub-list for extension extendee
	0,  // [0:43] is the sub-list for field type_name
}

func init() { file_api_proto_ai_v1_ai_proto_init() }
func file_api_proto_ai_v1_ai_proto_init() {
	if File_api_proto_ai_v1_ai_proto != nil {
		return
	}
	file_api_proto_ai_v1_ai_proto_msgTypes[0].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[1].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[2].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[3].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[4].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[5].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[7].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[8].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[10].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[11].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[12].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[17].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[18].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[19].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[24].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[26].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[27].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[29].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[31].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[33].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[37].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[38].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[40].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[42].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[43].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[44].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[45].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[46].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[47].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[49].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[51].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[52].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[54].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[60].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[61].OneofWrappers = []any{}
	file_api_proto_ai_v1_ai_proto_msgTypes[64].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_api_proto_ai_v1_ai_proto_rawDesc), len(file_api_proto_ai_v1_ai_proto_rawDesc)),
			NumEnums:      5,
			NumMessages:   65,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_api_proto_ai_v1_ai_proto_goTypes,
		DependencyIndexes: file_api_proto_ai_v1_ai_proto_depIdxs,
		EnumInfos:         file_api_proto_ai_v1_ai_proto_enumTypes,
		MessageInfos:      file_api_proto_ai_v1_ai_proto_msgTypes,
	}.Build()
	File_api_proto_ai_v1_ai_proto = out.File
	file_api_proto_ai_v1_ai_proto_goTypes = nil
	file_api_proto_ai_v1_ai_proto_depIdxs = nil
}
